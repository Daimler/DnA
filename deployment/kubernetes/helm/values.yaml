# Default values for helm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

#Suchart properties for frontend
frontend:
  enabled: true
  namespace: dna
  imagePullSecret:
    name: vddockerhub
    key: eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOnsidXNlcm5hbWUiOiJ2YXJkaGFuZGV2YWxsYSIsInBhc3N3b3JkIjoiZG9ja2VycmVnaXN0cnlAMDEiLCJlbWFpbCI6InZhcmRoYW5kZXZhbGxhQGdtYWlsLmNvbSIsImF1dGgiOiJkbUZ5WkdoaGJtUmxkbUZzYkdFNlpHOWphMlZ5Y21WbmFYTjBjbmxBTURFPSJ9fX0=
  ingress:
    enabled: false
    namespace: ingress
    host: ""
    lbIP: ""
    
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: ""
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  configJs: |
      window["INJECTED_ENVIRONMENT"]={
        OIDC_DISABLED: false,
        OIDC_PROVIDER: "OKTA",
        API_BASEURL: "api",
        CLIENT_IDS: "0oa54lsiggM54gE4b5d7",
        REDIRECT_URLS: "http://localhost:8080",
        OAUTH2_AUTH_URL: "https://dev-36980595.okta.com/oauth2/v1/authorize",
        OAUTH2_LOGOUT_URL: "https://dev-36980595.okta.com/oauth2/v1/logout",
        OAUTH2_REVOKE_URL: "https://dev-36980595.okta.com/oauth2/v1/revoke",
        OAUTH2_TOKEN_URL: "https://dev-36980595.okta.com/oauth2/v1/token",
        DNA_COMPANY_NAME: "DNA_AVD",
        DNA_APPNAME_HEADER: "DnA-MBC App",
        DNA_APPNAME_HOME: "Data and Analytics @MBC",
        DNA_CONTACTUS_HTML: "<div><p>There could be many places where you may need our help, and we are happy to support you. <br /> Please post your question(s) in our communication channels mentioned below.</p><p>Mattermost:&nbsp;<a href=\"https://matter.i.mercedes-benz.com/mercedes-benz/channels/dna-platform\" target=\"_blank\" rel=\"noreferrer\">https://matter.i.mercedes-benz.com/mercedes-benz/channels/dna-platform</a></p><p>Email:&nbsp;<a href=\"mailto:dna@daimler.com\">dna@daimler.com</a></p></div>",
        DNA_BRAND_LOGO_URL: "/images/branding/logo-brand.png",
        DNA_APP_LOGO_URL: "/images/branding/logo-app.png",
        ENABLE_INTERNAL_USER_INFO: false,
        ENABLE_DATA_COMPLIANCE: false,
        ENABLE_JUPYTER_WORKSPACE: true,
        JUPYTER_NOTEBOOK_URL: "http://localhost:9001",
        JUPYTER_NOTEBOOK_OIDC_POPUP_URL: "http://localhost:9001/hub/oauth_login?next=",
        JUPYTER_NOTEBOOK_OIDC_POPUP_WAIT_TIME: 5000,
        ENABLE_DATAIKU_WORKSPACE: true,
        DATAIKU_LIVE_APP_URL: "",
        DATAIKU_TRAINING_APP_URL: "",
        DATAIKU_FERRET_URL: "",
        ENABLE_MALWARE_SCAN_SERVICE: true,
        MALWARE_SCAN_SWAGGER_UI_URL: "http://localhost:9002/swagger-ui.html",
        ENABLE_MALWARE_SCAN_ONEAPI_INFO: true,
        ENABLE_DATA_PIPELINE_SERVICE: true,
        ENABLE_STORAGE_SERVICE: true,
        STORAGE_MFE_APP_URL: "http://localhost:7175",
        ENABLE_REPORTS: true,
        ENABLE_ML_PIPELINE_SERVICE: true,
        ENABLE_NOTIFICATION: true,
        DATA_PIPELINES_APP_BASEURL: "http://localhost:9010",
        DATA_PIPELINES_API_BASEURL: "http://localhost:9003/airflow/api",
        NOTIFICATIONS_API_BASEURL: "http://localhost:9004/api",
        DASHBOARD_API_BASEURL: "http://localhost:9005/api",
        MALWARESCAN_API_BASEURL: "http://localhost:9002/api",
        ML_PIPELINE_URL: "",
        MODEL_REGISTRY_API_BASEURL: "",
        INTERNAL_USER_TEAMS_INFO: '(Recommended to use Short ID. To find Short ID use <a href="" target="_blank" rel="noreferrer noopener">Teams</a>)'
      };

      window["STORAGE_INJECTED_ENVIRONMENT"]={
        CONTAINER_APP_URL: "http://localhost:8080",
        STORAGE_API_BASEURL: "http://localhost:7175/api",
        TOU_HTML: "<div>I agree to <a href=\"https://social.net/docs/DOC-467646\" target=\"_blank\" rel=\"noopener noreferrer\">terms of use</a></div>",
        ENABLE_DATA_CLASSIFICATION_SECRET: false,
        API_BASEURL: "http://localhost:8080/api"
      };
  
  probes:
    initialDelaySeconds: 30
    timeoutSeconds: 10
    periodSeconds: 10
    failureThreshold: 3

    readinessProbe:
      path: /
      port: 3000
      
  appFrontend:
    replicaCount: 1
    image: vardhandevalla/dna-app-fronted:fosstesting.oktav1
    ngnix:
      backend: http://dna-service.dna.svc.cluster.local:80
      grafanaServer: http://i3-monitoring-grafana.i3-monitoring.svc.cluster.local:80
      avscanMgwServer: http://dna-microgateway.clamav.svc.cluster.local:80

#Suchart properties for backend
backend:
  enabled: true
  namespace: dna
  imagePullSecret: 
    name: vddockerhub
    key: eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOnsidXNlcm5hbWUiOiJ2YXJkaGFuZGV2YWxsYSIsInBhc3N3b3JkIjoiZG9ja2VycmVnaXN0cnlAMDEiLCJlbWFpbCI6InZhcmRoYW5kZXZhbGxhQGdtYWlsLmNvbSIsImF1dGgiOiJkbUZ5WkdoaGJtUmxkbUZzYkdFNlpHOWphMlZ5Y21WbmFYTjBjbmxBTURFPSJ9fX0= 
  ingress:
    enabled: false
    namespace: ingress
    host: ""
    lbIP: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  app:
    backend:
      replicaCount: 1
      
      image: vardhandevalla/dna-app-backend:fosstesting.oktav1
      #Specify the secrets which will be used by backend to run properly, Secrets will be encoded in base64 at the time of deployment
      secrets: 
        name: app-secrets
        notebookSecretToken: b9cfc64284c1c5950812a0745da0bc274033b7ef538a07950a17e9c69ee9e388
        oidcClientID: 0oa55doiaeNNjCciu5d7
        oidcClientSecret: G_vUbDjcPvHbPODZ0PLjDvbl8Fb1GHATRqfIlmr5
        drdCertPassword: a7PQw01Tzx4X9Lso8A6M
        jwtSecretKey: oeRaYY7Wo24sDqKSX3IM9ASGmdGPmkTd9jo1QTy4b7P9Ze5_9hKolVX8xNrQDcNRfVEdTZNOuOyqEGhXEbdJI-ZQ19k_o9MI0y3eZN2lp9jow55FfXMiINEdt1XR85VipRLSOkT6kSpzs2x-jbLDiz9iFVzkd81YKxMgPA7VfZeQUm4n-mOmnWMaVX30zGFU4L3oPBctYKkl4dYfqYWqRNfrgPJVi5DGFjywgxx0ASEiJHtV72paI3fDR2XwlSkyhhmY-ICjCRmsJN4fX1pdoL8a18-aQrvyu4j0Os6dVPYIoPvvY0SAZtWYKHfM15g7A3HD4cVREf9cUsprCRK93w
        s3AccessKey: admin
        s3SecretKey: S6O!#uiOop
        dataikuProdApiKey: ""
        dataikuTrainingApiKey: "" 
        avscanApiKey: a95b841c-1bd7-4f15-a7a8-ddd7222fe523
        appDBUserName: postgres
        appDBPassword: postgres
      config:
        enableItsmm: false
        enableJupyterNotebook: true
        enableDataiku: false
        enableAttachmentScan: true
        enableInternalUser: false
        redirectUrl: http://localhost:8080
        dbUri: jdbc:postgresql://dna-bitnamipostgresql:5432/db
        oidcUserInfoUrl: https://dev-36980595.okta.com/oauth2/v1/userinfo
        oidcTokenIntrospectionUrl: https://dev-36980595.okta.com/oauth2/v1/introspect
        oidcProvider: OKTA
        oidcTokenRevocationUrl: https://dev-36980595.okta.com/oauth2/v1/revoke
        internalUserRequestUrl: ""
        internalCertFile: ""
        oidcDisabled: false
        s3BucketName: dna
        s3Url: http://minio.storage.svc.cluster.local:9000
        corosOriginUrl: http://*
        jupyterNotebookUrl: http://proxy-public.notebooks.svc.cluster.local:8000/hub/api/users
        dataikuProdUri: ""
        dataikuProdAdminGroup: "" 
        dataikutraininguri: ""
        avscanAppId: 479c814b-2c32-4eea-8c8f-77bc423811b6
        naasBroker: my-release-kafka-headless.kafka.svc.cluster.local:9092
        loggingEnvironment: dev
        loggingPath: /tmp/app/log
        dataikuProjectUri: /projects/
        apacCorpDir: ""
        emeaCorpDir: ""
        dataikuUserRoleUriPath: ""
        dataikuPermissionUriPath:
        dataikuTrainingAdminGroup: ""
        avscanUri: http://clamav-rest-service.clamav.svc.cluster.local:8181/avscan/api/v1
        flywayBaseline: "true"
        flywayBaselineOnMigrate: "true"
        flywayBaselineVersion: "0"
        flywaySchema: "public"
        dashboardUri: http://dashboard-backend-service.dashboard.svc.cluster.local:7173/dashboards

  probes:
    initialDelaySeconds: 120
    timeoutSeconds: 30
    periodSeconds: 10
    failureThreshold: 3
    livenessProbe:
      path: /actuator/health/liveness
      port: 7171
    readinessProbe:
      path: /actuator/health/readiness
      port: 7171 

#Suchart properties for i3postgressql database
i3postgresql:
  enabled: false
  namespace: dna
  app:
    db:
      replicaCount: 1
      image: postgresql:10.3-2.1.0
      pgssl: "NO"
      dbname: db
      secrets: 
        name: postgres-secrets
        backupUserName: ""
        backupUserPassword: ""
        dbAdminUserName: ""
        dbAdminPassword: ""
        patroniUserPassword: ""
        appUserName: ""
        appPassword: ""
      probes:
        initialDelaySeconds: 30
        timeoutSeconds: 10
        periodSeconds: 10
        failureThreshold: 3
        readinessProbe:
          port: "" 
      airflowDB:
        airflowUserName: ""
        airflowdbName: ""
        airflowdbPwd: ""
      dashboardDB:
        dashboardUserName: ""
        dashboarddbName: ""
        dashboarddbPwd: "" 

#Subchart properties for bitnami postgresql
bitnamipostgresql:
  enabled: true
  namespace: dna
  global:
    postgresql:
      postgresqlDatabase: db
      postgresqlUsername: postgres
      postgresqlPassword: postgres
      servicePort: 5432

#Subchart properties for clamav
clamav:
  enabled: true
  appName: clamav
  namespace: clamav
    
  securityContext:
    runasUser: 0 
  app:
    backend:
      name: clamav-rest
      image: vardhandevalla/dna-malware-backend:fosstesting.oktav1
      replicaCount: 1
      secrets:
        name: oneapi-secrets
        onapiBasicAuthToken: ""
        jwtSecretKey: oeRaYY7Wo24sDqKSX3IM9ASGmdGPmkTd9jo1QTy4b7P9Ze5_9hKolVX8xNrQDcNRfVEdTZNOuOyqEGhXEbdJI-ZQ19k_o9MI0y3eZN2lp9jow55FfXMiINEdt1XR85VipRLSOkT6kSpzs2x-jbLDiz9iFVzkd81YKxMgPA7VfZeQUm4n-mOmnWMaVX30zGFU4L3oPBctYKkl4dYfqYWqRNfrgPJVi5DGFjywgxx0ASEiJHtV72paI3fDR2XwlSkyhhmY-ICjCRmsJN4fX1pdoL8a18-aQrvyu4j0Os6dVPYIoPvvY0SAZtWYKHfM15g7A3HD4cVREf9cUsprCRK93w
        appDBUserName: malware
        appDBPassword: malware123
      config: 
        clamav_backend_url: clamav-service
        clamav_backend_port: 3310
        max_file_size: 4000MB
        max_request_size: 4000MB
        api_request_limit: 20
        with_in: 2
        time_unit: seconds
        auth_api_host: http://dna-service.dna.svc.cluster.local:80/api/subscription
        restricted_url_pattern: api/v1/*
        loggingPath: /tmp/clamav/log
        loggingEnvironment: dev
        corsOriginUrl: http://*
        dbUri: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/malware
        flywayEnabled: true
        flywayBaselineOnMigrate: true
        flywayBaselineVersion: 0
        flywaySchema: public
        vaultHost: vault.vault.svc.cluster.local
        vaultPort: 8200
        appUrl: http://dna-service.dna.svc.cluster.local:80
        enableAuth: true
        verifyLoginApi: /api/verifyLogin
        unscribeMalwareScanApi: /api/malwarescan/unsubscribe/
        naas_broker: my-release-kafka-headless.kafka.svc.cluster.local:9092

    vault:
      secret:
        rootToken: hvs.3FpK0LNiEULFPuuyAOKlWJiv

    probes:
      initialDelaySeconds: 60
      timeoutSeconds: 10
      periodSeconds: 10
      failureThreshold: 3
      livenessProbe:
        path: /actuator/health/liveness
        port: api
      
      readinessProbe:
        path: /actuator/health/readiness
        port: api

  image:
    repo: vardhandevalla/clamav:latest
    replicaCount: 1
    pullPolicy: Always
  ingress:
    enabled: false
    host: "" 
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: "" 
  imagePullSecret: 
    name: vddockerhub

  Storage:
    storageClass: managed
    accessModes: ReadWriteOnce
    size: 2G

  resources:
    requests:
      memory: "5000Mi"
      cpu: "500m"
    limits:
      memory: "8000Mi"
      cpu: "700m"
      
#Subchart properties for naas
naas:
  enabled: true
  appName: naas
  namespace: naas
  app:
    backend:
      image: vardhandevalla/dna-naas-backend:fosstesting.oktav1
      secrets:
        name: naas-app-secrets
        jwtKey: oeRaYY7Wo24sDqKSX3IM9ASGmdGPmkTd9jo1QTy4b7P9Ze5_9hKolVX8xNrQDcNRfVEdTZNOuOyqEGhXEbdJI-ZQ19k_o9MI0y3eZN2lp9jow55FfXMiINEdt1XR85VipRLSOkT6kSpzs2x-jbLDiz9iFVzkd81YKxMgPA7VfZeQUm4n-mOmnWMaVX30zGFU4L3oPBctYKkl4dYfqYWqRNfrgPJVi5DGFjywgxx0ASEiJHtV72paI3fDR2XwlSkyhhmY-ICjCRmsJN4fX1pdoL8a18-aQrvyu4j0Os6dVPYIoPvvY0SAZtWYKHfM15g7A3HD4cVREf9cUsprCRK93w
        db: 
          appUserName: postgres
          appPassword: postgres
      config:
        api_db_url: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/db
        naas_broker: my-release-kafka-headless.kafka.svc.cluster.local:9092
        max_poll_records: 6000
        dna_uri: http://dna-service.dna.svc.cluster.local:80
        dna_auth_enable: true
        mailServerHost: ""
        mailServerPort: ""
        notificationSenderEmail: "" 
        poll_time: 5000
        naas_central_topic: CentralEventTopic
        naas_centralread_topic: CentralReadTopic
        naas_centraldelte_topic: CentralDeleteTopic
        loggingPath: /tmp/naas/log
        loggingEnvironment: dev
        corsOriginUrl: http://*
    # resources:
    #   requests:
    #     memory: "512Mi"
    #     cpu: "250m"
    #   limits:
    #     memory: "1000Mi"
    #     cpu: "500m"
    probes:
      initialDelaySeconds: 180
      timeoutSeconds: 10
      periodSeconds: 30
      failureThreshold: 3
      livenessProbe:
        path: /actuator/health/liveness
        port: api
      
      readinessProbe:
        path: /actuator/health/readiness
        port: api
  
  image:
    pullPolicy: Always
      
  ingress:
    enabled: false
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  imagePullSecret: 
   name: vddockerhub
  
#Subchart properties for notebooks
notebooks:
  enabled: true 
  namespace: notebooks
  app:
    jupyter:
      config:
        configproxy_auth_token: cffebc4d8df0ce2be5d020d0a240ca6c237d1d49f88de5f3594351f875e1c6ad
        kf_pipelines_endpoint: ""

    image:
      name: vardhandevalla/jupyterhub:vokta.dev
      pullPolicy: Always

    profileListImages:
      default: vardhandevalla/pyspark-notebook-default:vokta.dev
      tensorflow: vardhandevalla/pyspark-notebook-tensorflow:vokta.dev
      chronos: ""

    proxy:
      image: vardhandevalla/configurable-http-proxy:vokta.dev

    hubConfig:
      name: hub-config
      KubeSpawnerimage: vardhandevalla/pyspark-notebook-default:vokta.dev
      serviceAccount: hub 
      securitycontext: HOST http://localhost:8080/* http://localhost:9001 https://dev-36980595.okta.com
      #BY default jupyter notebook will not have the okta authentication , we need to configure the generic oAuth authenticator
      oauthAuthenticator: GenericOAuthenticator
      oauthClientId: 0oa5em9fqg92V7Xyf5d7
      oauthClientSecret: fTjokQtY1wrY2CRyczw2fdGyohlz-YyJNhVpqxkE
      oauthCallback: http://localhost:9001/hub/oauth_callback
      oauthAuthorizeUrl: https://dev-36980595.okta.com/oauth2/v1/authorize
      oauthTokenUrl: https://dev-36980595.okta.com/oauth2/v1/token
      oauthUserDataUrl: https://dev-36980595.okta.com/oauth2/v1/userinfo
      oauthUsrKey: sub
      oauthLoginSvc: OKTA
      prespawn_hook: ""
      enableUserNS: "False"
      userNameSpaceTemplate: kubeflow

  ingress:
    enabled: false
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""

  Storage:
    storageClass: default
    accessModes: ReadWriteOnce
    size: 1G

  resources:
    cpu: 1
    memory: 4G
  
  # resources:
  #   cpu: 1
  #   memory: 4G

#Subchart properties for dashboard
dashboard:
  enabled: true
  namespace: dashboard
  image: vardhandevalla/dna-dashboard-backend:fosstesting.oktav1
  imagePullSecret: 
    name: vddockerhub
  dbUrl: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/dashboard
  secret:
    name: dashboard-secrets
    appUserName: dashboard
    appPassword: dashboard
    jwtKey: oeRaYY7Wo24sDqKSX3IM9ASGmdGPmkTd9jo1QTy4b7P9Ze5_9hKolVX8xNrQDcNRfVEdTZNOuOyqEGhXEbdJI-ZQ19k_o9MI0y3eZN2lp9jow55FfXMiINEdt1XR85VipRLSOkT6kSpzs2x-jbLDiz9iFVzkd81YKxMgPA7VfZeQUm4n-mOmnWMaVX30zGFU4L3oPBctYKkl4dYfqYWqRNfrgPJVi5DGFjywgxx0ASEiJHtV72paI3fDR2XwlSkyhhmY-ICjCRmsJN4fX1pdoL8a18-aQrvyu4j0Os6dVPYIoPvvY0SAZtWYKHfM15g7A3HD4cVREf9cUsprCRK93w
  appUrl: http://dna-service.dna.svc.cluster.local:80
  enableAuth: true
  loggingPath: /tmp/dashboard/log
  loggingEnvironment: dev
  flywayBaseline: "true"
  flywayBaselineOnMigrate: "true"
  flywayBaselineVersion: "0"
  flywaySchema: "public"
  containerPort: 7173
  corsOriginUrl: http://*
  naasBroker: my-release-kafka.kafka.svc.cluster.local:9092
  ingress:
    enabled: false
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  # resources:
  #   requests:
  #     memory: "512Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "1000Mi"
  #     cpu: "500m"

  probes:
    initialDelaySeconds: 120
    timeoutSeconds: 10
    periodSeconds: 20
    failureThreshold: 3
    livenessProbe:
      path: /actuator/health/liveness
      port: api
        
    readinessProbe:
      path: /actuator/health/readiness
      port: api

#Subchart properties for airflow
airflow:
  enabled: true
  appName: airflow
  namespace: airflow
  imagePullSecret: 
    name: vddockerhub
    key: eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOnsidXNlcm5hbWUiOiJ2YXJkaGFuZGV2YWxsYSIsInBhc3N3b3JkIjoiZG9ja2VycmVnaXN0cnlAMDEiLCJlbWFpbCI6InZhcmRoYW5kZXZhbGxhQGdtYWlsLmNvbSIsImF1dGgiOiJkbUZ5WkdoaGJtUmxkbUZzYkdFNlpHOWphMlZ5Y21WbmFYTjBjbmxBTURFPSJ9fX0= 
  backend:
    image: vardhandevalla/airflow_backend:v1okta.dev
    imagePullPolicy: Always
    dbUrl: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/airflow
    secret:
      name: airflow-backend-secrets
      dbPassword: airflow
      dbUsername: airflow
      gitToken: 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 
      jwtKey: oeRaYY7Wo24sDqKSX3IM9ASGmdGPmkTd9jo1QTy4b7P9Ze5_9hKolVX8xNrQDcNRfVEdTZNOuOyqEGhXEbdJI-ZQ19k_o9MI0y3eZN2lp9jow55FfXMiINEdt1XR85VipRLSOkT6kSpzs2x-jbLDiz9iFVzkd81YKxMgPA7VfZeQUm4n-mOmnWMaVX30zGFU4L3oPBctYKkl4dYfqYWqRNfrgPJVi5DGFjywgxx0ASEiJHtV72paI3fDR2XwlSkyhhmY-ICjCRmsJN4fX1pdoL8a18-aQrvyu4j0Os6dVPYIoPvvY0SAZtWYKHfM15g7A3HD4cVREf9cUsprCRK93w
      oidcClientID: 0oa5b1yg9kGa4sIvl5d7
      oidcClientSecret: 7VtOFzfl8WSwBuFMn_9I1eyAa68QHLpDhHD1p8IR
    containerPort: 7171
    crossOriginUrl: http://*
    apiUrl: http://dna-service.dna.svc.cluster.local:80
    oidcInfoUrl: https://dev-36980595.okta.com/oauth2/v1/userinfo 
    oidcIntrospectionUrl: https://dev-36980595.okta.com/oauth2/v1/introspect 
    oidcRevocationUrl: https://dev-36980595.okta.com/oauth2/v1/revoke 
    oidcDisabled: false
    gitUrl: https://vardhandevalla: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@github.com/Vardhandevalla/airflow_dags.git
    gitMountPath: /git/airflow_dags
    #i deleted the airflow-user-dags to airflow_dags
    gitBranch: main
    dag:
      path: dags
      ext: py
      waitTime: 20
      retry: 20
    loggingPath: /tmp/airflow/log
    loggingEnvironment: dev
    flywayBaseline: "true"
    flywayBaselineOnMigrate: "true"
    flywayBaselineVersion: "0"
    flywaySchema: "public"
  ##Ingress for airflow backend service
  ingress:
    enabled: false
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  ##Ingress for the airflow Application
  airflow:
    ingress:
      enabled: false
      host: ""
      annotations:
        traefik.frontend.rule.type: PathPrefix
        kubernetes.io/ingress.class: traefik
        traefik.ingress.kubernetes.io/router.tls: "true"
        traefik.ingress.kubernetes.io/router.entrypoints: websecure
        cert-manager.io/cluster-issuer: ""
  #dockeraccount key value which is encoded value of docker.configjson
  pullSecretData: eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOnsidXNlcm5hbWUiOiJ2YXJkaGFuZGV2YWxsYSIsInBhc3N3b3JkIjoiZG9ja2VycmVnaXN0cnlAMDEiLCJlbWFpbCI6InZhcmRoYW5kZXZhbGxhQGdtYWlsLmNvbSIsImF1dGgiOiJkbUZ5WkdoaGJtUmxkbUZzYkdFNlpHOWphMlZ5Y21WbmFYTjBjbmxBTURFPSJ9fX0=
  secret:
    gitUserName: vardhandevalla
    gitPassword: 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
    #gitSshKey: ""
    knownHosts: ""
    postgresql:
      sqlAlchemyConn: postgresql+psycopg2://airflow:airflow@dna-bitnamipostgresql.dna.svc.cluster.local:5432/airflow
    clientSecret: CSAgewogICAgIndlYiI6IHsKICAgICAgICAiY2xpZW50X2lkIjogIjBvYTVoczQxNXRqeDNveW9LNWQ3IiwKICAgICAgICAiY2xpZW50X3NlY3JldCI6ICJpVEVXVll2WXpwOXYxRkVHa3hKOTFOanR4ZmFZTDZlRUMteDZOT0Y5IiwKICAgICAgICAiYXV0aF91cmkiOiAiaHR0cHM6Ly9kZXYtMzY5ODA1OTUub2t0YS5jb20vb2F1dGgyL3YxL2F1dGhvcml6ZSIsCiAgICAgICAgInRva2VuX3VyaSI6ICJodHRwczovL2Rldi0zNjk4MDU5NS5va3RhLmNvbS9vYXV0aDIvdjEvdG9rZW4iLAogICAgICAgICJ1c2VyaW5mb191cmkiOiAiaHR0cHM6Ly9kZXYtMzY5ODA1OTUub2t0YS5jb20vb2F1dGgyL3YxL3VzZXJpbmZvIiwKICAgICAgICAiaXNzdWVyIjogImh0dHBzOi8vZGV2LTM2OTgwNTk1Lm9rdGEuY29tIiwKICAgICAgICAicmVkaXJlY3RfdXJpcyI6IFsiaHR0cDovL2xvY2FsaG9zdDo5MDEwLyoiXQogICAgfQp9
    #clientsecret is a json value of all our client credentials
    # {
    # "web": {
    #     "client_id": "0oa5hs415tjx3oyoK5d7",
    #     "client_secret": "iTEWVYvYzp9v1FEGkxJ91NjtxfaYL6eEC-x6NOF9",
    #     "auth_uri": "https://dev-36980595.okta.com/oauth2/v1/authorize",
    #     "token_uri": "https://dev-36980595.okta.com/oauth2/v1/token",
    #     "userinfo_uri": "https://dev-36980595.okta.com/oauth2/v1/userinfo",
    #     "issuer": "https://dev-36980595.okta.com",
    #     "redirect_uris": ["http://localhost:9010/*"]
    # }
    #   }
  gitSync:
    image: vardhandevalla/gitsync-air-se:test2
    repo: https://github.com/Vardhandevalla/airflow_dags.git
    dest: git-sync
    branch: main
    ssh: "false"
    root: /git

  airflowIngressRoute:
    enabled: false 
  airflowMiddleware: 
    enabled: false
  # editor:
  #   gitEnabled: false
  #   gitCMD: /usr/bin/git
  #   gitDefaultArgs: -c color.ui=true
  #   gitIntRepo: False
  #   lineLength: 88
  #   stringNormalization: False
    
  configuration:
    loggingLevel: INFO
    executor: KubernetesExecutor
    parallelism: 32
    pluginsFolder: /usr/local/airflow/plugins
    loadExamples: False
    scheduler:
      dagDirListInterval: 5
      childProcessLogDirectory: /usr/local/airflow/logs/scheduler
      jobHeartbeatSec: 5
      parsingProcesses: 2
      schedulerHeartbeatSec: 5
      minFileProcessInterval: 0
      statsdOn: False
      statsdHost: localhost
      statsdPort: 8125
      statsdPrefix: airflow
      minFileParsingLoopTime: 1
      printStatsInterval: 30
      schedulerZombieTaskThreshold: 300
      maxTisPerQuery: 0
      authenticate: False
      catchupByDefault: True
    webserver:
      baseUrl: http://localhost:9010
      path: /
      rbac: True
      host: 0.0.0.0
      port: 8080
      masterTimeout: 120
      workerTimeout: 120
      workerRefreshBatchSize: 1
      workerRefreshInterval: 30
      secretKey: 18a58f846390896b2ce002f66af04d2c99bcb8fafabefff6c86e2bbe20ae
      numberOfWorkers: 4
      workerClass: sync
      exposeConfig: True
      dagDefaultView: graph
      dagOrientation: LR
      demoMode: False
      logFetchTimeoutSec: 5
      hidePausedDagsByDefault: False
      pageSize: 100
    kubernetes:
      workerContainerImagePullPolicy: Always
      workerServiceAccountName: airflow
      deleteWorkerPods: True
      dagsInImage: false
      gitSubpath: dags
      inCluster: True
      gitSyncContainerRepository: docker.io/vardhandevalla/gitsync-air-se
      gitSyncContainerTag: latest
      gitSyncInitContainerName: git-sync-container
      gitSyncRunAsUser: 1000
      runAsUser: 1000
      fsGroup: 65533
    kubernetesLabels:
      airflowWorker:

  docker:
    image:
      name: vardhandevalla/airflow-frontend
      tag: vokta.dev

  service:
    port: 8080

  db:
    port: 5432

  webserver:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 2000Mi
      cpu: 1000m

  scheduler:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 2000Mi
      cpu: 1000m

  gitContainer:
    requests:
      memory: 250Mi
      cpu: 250m
    limits:
      memory: 1000Mi
      cpu: 500m

  backendResources:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 1000Mi
      cpu: 500m

  volumes:
    logsPath: /usr/local/airflow/logs
    dagsPath: /usr/local/airflow/dags/git-sync/dags
    gitDagsPath: /usr/local/airflow/dags
    airflowLogsClaim:
      resourcePolicy: keep
      accessMode: ReadWriteOnce
      storage:
        className: ""
        size: 2Gi

  oidc:
    logout:
      uri: https://dev-36980595.okta.com/oauth2/v1/logout

  Storage:
    storageClass: ""
    accessModes: ReadWriteOnce
    size: 1Gi
  probes:
    initialDelaySeconds: 120
    timeoutSeconds: 10
    periodSeconds: 10
    failureThreshold: 3
    livenessProbe:
      path: /airflow/actuator/health/liveness
      port: api-http
      
    readinessProbe:
      path: /airflow/actuator/health/readiness
      port: api-http
  
#Subchart properties for microgateway

microgateway:
  enabled: false
  namespace: clamav
  proxy: ""
  noProxy: ""
  apigee:
    # environment variables 
    debug: "*" # Enable DEBUG mode with "*"
    key: ""
    secret: ""
    org: internal
    env: development
    # if introspection is required add the introspection credentials
    #introspection_client_id: 
    #introspection_client_secret: 
    
    # validate target https certificates (1=enabled; 0=disabled)
    node_tls_reject_unauthorized: 1

    certs:
      # key and cert will be mount under /home/node/certs/[host].key|.cert
      # - host: example.org
      #  key: put base64 encoded key here
      #  cert: put base64 encoded certificate here
    config:
      # content of apigee config. Make sure that the whole content has the correct indent of two spaces!
      # edge_config, analytics and oauth is already defined
      edgemicro:
      port: 8080
      max_connections: 1000
      max_connections_hard: 5000
      max_times: 300
      config_change_poll_interval: 86400
      logging:
        to_console: true
        level: debug
        stack_trace: false
      plugins:
        sequence:
          - cors-oneapi
          - spikearrest
          #- introspection
          # ApiKey Security needs 'oauth' plugin. Confusing. I know.
          - oauth
          #- quota
          #- app-to-header
          - backend-basicauth
          #- backend-jwt
      proxies:
        # References an Apigee Proxy Configuration
        ###################################
        # !!! REPLACE WITH YOUR PROXY !!! #
        ###################################
        - edgemicro_malwarescanapi_v1
      # In case a proxy is needed for accessing the API backend (target-server)
      proxy:
        url: ""
        enabled: false
    headers:
      x-forwarded-for: true
      x-forwarded-host: true
      x-request-id: true
      x-response-time: true
      via: true
    backend-basicauth:
      username: 'admin'
      password: 'password123'
    cors-oneapi:
      cors-allow-credentials: true
    backend-jwt:
      header_attribute_name: x-claims
      sign_secret: 'my-secret'
      claims:
        - iss
        - sub
        - client_id
        - scope
        - app_name
        - custom_client_identification
    spikearrest:
      timeUnit: minute
      allow: 6000
      bufferSize: 600
    # client certificate configuration
    # targets:
    #   - host: 'example.org'
    #     ssl:
    #       client:
    #         key: /home/node/certs/example.org.key # Don't change this, will be set via certs.key
    #         cert: /home/node/certs/example.org.crt # Don't change this, will be set via certs.cert
    #         passphrase: 'optional'

  image:
    repository: edgemicro
    tag: latest
    pullPolicy: IfNotPresent
  
  nameOverride: ""
  fullnameOverride: ""

  service:
    type: NodePort
    port: 80
    nodePort: 30005

  # resources:
  #   # We usually recommend not to specify default resources and to leave this as a conscious
  #   # choice for the user. This also increases chances charts run on environments with little
  #   # resources, such as Minikube. If you do want to specify resources, uncomment the following
  #   # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  #   limits:
  #     cpu: 100m
  #     memory: 128Mi
  #   requests:
  #     cpu: 100m
  #     memory: 128Mi

  # Optionaly an ingress route can be defined. Routing options are configured in here.
  ingress:
    enabled: true
    basePath: /malware-scan/api/v1
    # Define a list of hosts for the routing. If an empty list is provided routing will be enabled for all hostnames
    hosts: [""]
    annotations: {}
    #traefik.ingress.kubernetes.io/rewrite-target: /malware_scan_api
    # Add custom labels to ingress route
    labels: {}

#Subchart properties for vault
vault:
  enabled: true 
  namespace: vault
  global:
    # enabled is the master enabled switch. Setting this to true or false
    # will enable or disable all the components within this chart by default.
    enabled: true
    # Image pull secret to use for registry authentication.
    imagePullSecrets:
    # imagePullSecrets:
       - name: vddockerhub
    # TLS for end-to-end encrypted transport
    tlsDisable: true

  server:
    # Resource requests, limits, etc. for the server cluster placement. This
    # should map directly to the value of the resources field for a PodSpec.
    # By default no direct resource request is made.

    image:
      repository: docker.io/vardhandevalla/vault
      tag: latest
      # Overrides the default Image Pull Policy
      pullPolicy: IfNotPresent

    resources:
    # resources:
    #   requests:
    #     memory: 256Mi
    #     cpu: 250m
    #   limits:
    #     memory: 256Mi
    #     cpu: 250m

    # Ingress allows ingress services to be created to allow external access 
    # from Kubernetes to access Vault pods.
    ingress:
      enabled: false
      labels: { }
      # traffic: external
      annotations: { }
        # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      hosts:
        - host: chart-example.local
          # As of now vault can only be servered on "/"
          paths: [ / ]

      tls: [ ]
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local


    # authDelegator enables a cluster role binding to be attached to the service
    # account.  This cluster role binding can be used to setup Kubernetes auth
    # method.  https://www.vaultproject.io/docs/auth/kubernetes.html
    authDelegator:
      enabled: false

    # extraContainers is a list of sidecar containers. Specified as a raw YAML string.
    extraContainers: null


    # Used to define custom readinessProbe settings
    readinessProbe:
      enabled: true
      # If you need to use a http path instead of the default exec
      # path: /v1/sys/health?standbyok
    # Used to enable a livenessProbe for the pods
    livenessProbe:
      enabled: false
      path: /v1/sys/health?standbyok

    # extraEnvironmentVars is a list of extra enviroment variables to set with the stateful set. These could be
    # used to include variables required for auto-unseal.
    extraEnvironmentVars: { }
      # GOOGLE_REGION: global
      # GOOGLE_PROJECT: myproject
    # GOOGLE_APPLICATION_CREDENTIALS: /vault/userconfig/myproject/myproject-creds.json

    # extraSecretEnvironmentVars is a list of extra enviroment variables to set with the stateful set.
    # These variables take value from existing Secret objects.
    extraSecretEnvironmentVars: [ ]
      # - envName: AWS_SECRET_ACCESS_KEY
      #   secretName: vault
    #   secretKey: AWS_SECRET_ACCESS_KEY

    # extraVolumes is a list of extra volumes to mount. These will be exposed
    # to Vault in the path `/vault/userconfig/<name>/`. The value below is
    # an array of objects, examples are shown below.
    extraVolumes: [ ]
      # - type: secret (or "configMap")
      #   name: my-secret
    #   path: null # default is `/vault/userconfig`

    # Affinity Settings
    # Commenting out or setting as empty the affinity variable, will allow
    # deployment to single node services such as Minikube
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: {{ template "vault.name" . }}
                app.kubernetes.io/instance: "{{ .Release.Name }}"
                component: server
            topologyKey: kubernetes.io/hostname

    # Toleration Settings for server pods
    # This should be a multi-line string matching the Toleration array
    # in a PodSpec.
    tolerations: { }

    # nodeSelector labels for server pod assignment, formatted as a muli-line string.
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    # Example:
    # nodeSelector: |
    #   beta.kubernetes.io/arch: amd64
    nodeSelector: { }

    # Extra labels to attach to the server pods
    # This should be a multi-line string mapping directly to the a map of
    # the labels to apply to the server pods
    extraLabels: { }

    # Extra annotations to attach to the server pods
    # This should be a multi-line string mapping directly to the a map of
    # the annotations to apply to the server pods
    annotations: { }

    # Enables a headless service to be used by the Vault Statefulset
    service:
      enabled: true
      # clusterIP controls whether a Cluster IP address is attached to the
      # Vault service within Kubernetes.  By default the Vault service will
      # be given a Cluster IP address, set to None to disable.  When disabled
      # Kubernetes will create a "headless" service.  Headless services can be
      # used to communicate with pods directly through DNS instead of a round robin
      # load balancer.
      # clusterIP: None

      # Port on which Vault server is listening
      port: 8200
      # Target port to which the service should be mapped to
      targetPort: 8200
      # Extra annotations for the service definition
      annotations: { }

    # This configures the Vault Statefulset to create a PVC for data
    # storage when using the file backend.
    # See https://www.vaultproject.io/docs/configuration/storage/index.html to know more
    dataStorage:
      enabled: true
      # Size of the PVC created
      size: 1Gi
      # Name of the storage class to use.  If null it will use the
      # configured default Storage Class.
      storageClass: managed
      # Access Mode of the storage device being used for the PVC
      accessMode: ReadWriteOnce

    # This configures the Vault Statefulset to create a PVC for audit
    # logs.  Once Vault is deployed, initialized and unseal, Vault must
    # be configured to use this for audit logs.  This will be mounted to
    # /vault/audit
    # See https://www.vaultproject.io/docs/audit/index.html to know more
    auditStorage:
      enabled: true
      # Size of the PVC created
      size: 1Gi
      # Name of the storage class to use.  If null it will use the
      # configured default Storage Class.
      storageClass: managed
      # Access Mode of the storage device being used for the PVC
      accessMode: ReadWriteOnce

    # Run Vault in "dev" mode. This requires no further setup, no state management,
    # and no initialization. This is useful for experimenting with Vault without
    # needing to unseal, store keys, et. al. All data is lost on restart - do not
    # use dev mode for anything other than experimenting.
    # See https://www.vaultproject.io/docs/concepts/dev-server.html to know more
    dev:
      enabled: false

    # Run Vault in "standalone" mode. This is the default mode that will deploy if
    # no arguments are given to helm. This requires a PVC for data storage to use
    # the "file" backend.  This mode is not highly available and should not be scaled
    # past a single replica.
    standalone:
      enabled: "-"

      # config is a raw string of default configuration when using a Stateful
      # deployment. Default is to use a PersistentVolumeClaim mounted at /vault/data
      # and store data there. This is only used when using a Replica count of 1, and
      # using a stateful set. This should be HCL.
      config: |
        ui = true

        listener "tcp" {
          tls_disable = 1
          address = "[::]:8200"
          cluster_address = "[::]:8201"
        }
        storage "file" {
          path = "/vault/data"
        }

        # Example configuration for using auto-unseal, using Google Cloud KMS. The
        # GKMS keys must already exist, and the cluster must have a service account
        # that is authorized to access GCP KMS.
        #seal "gcpckms" {
        #   project     = "vault-helm-dev"
        #   region      = "global"
        #   key_ring    = "vault-helm-unseal-kr"
        #   crypto_key  = "vault-helm-unseal-key"
        #}

    # Run Vault in "HA" mode. There are no storage requirements unless audit log
    # persistence is required.  In HA mode Vault will configure itself to use Consul
    # for its storage backend.  The default configuration provided will work the Consul
    # Helm project by default.  It is possible to manually configure Vault to use a
    # different HA backend.
    ha:
      enabled: false
      replicas: 3

      # config is a raw string of default configuration when using a Stateful
      # deployment. Default is to use a Consul for its HA storage backend.
      # This should be HCL.
      config: |
        ui = true

        listener "tcp" {
          tls_disable = 1
          address = "[::]:8200"
          cluster_address = "[::]:8201"
        }
        storage "consul" {
          path = "vault"
          address = "HOST_IP:8500"
        }

        # Example configuration for using auto-unseal, using Google Cloud KMS. The
        # GKMS keys must already exist, and the cluster must have a service account
        # that is authorized to access GCP KMS.
        #seal "gcpckms" {
        #   project     = "vault-helm-dev-246514"
        #   region      = "global"
        #   key_ring    = "vault-helm-unseal-kr"
        #   crypto_key  = "vault-helm-unseal-key"
        #}

      # A disruption budget limits the number of pods of a replicated application
      # that are down simultaneously from voluntary disruptions
      disruptionBudget:
        enabled: true

        # maxUnavailable will default to (n/2)-1 where n is the number of
        # replicas. If you'd like a custom value, you can specify an override here.
        maxUnavailable: null

    # Definition of the serviceAccount used to run Vault.
    serviceAccount:
      annotations: { }

  # Vault UI
  ui:
    # True if you want to create a Service entry for the Vault UI.
    #
    # serviceType can be used to control the type of service created. For
    # example, setting this to "LoadBalancer" will create an external load
    # balancer (for supported K8S installations) to access the UI.
    enabled: false
    serviceType: "ClusterIP"
    serviceNodePort: null
    externalPort: 8200

    # loadBalancerSourceRanges:
    #   - 10.0.0.0/16
    #   - 1.78.23.3/32

    # loadBalancerIP:

    # Extra annotations to attach to the ui service
    # This should be a multi-line string mapping directly to the a map of
    # the annotations to apply to the ui service
    annotations: { }

#subchart  value for  storageMfe
storagemfe:
  enabled: true 
  namespace: storage
  configJs: |
    window["STORAGE_INJECTED_ENVIRONMENT"]={
        CONTAINER_APP_URL: "http:localhost:8080",
        STORAGE_API_BASEURL: "http:localhost:7175/api",
        TOU_HTML: "<div>I agree to <a href=\"https://social/docs/DOC-467646\" target=\"_blank\" rel=\"noopener noreferrer\">terms of use</a></div>", 
        ENABLE_DATA_CLASSIFICATION_SECRET: false,
        API_BASEURL: "http://localhost:8080/api"
        TRINO_API_BASEURL: "http://localhost:7575/api"
      }; 
  conf:
    storageBackendUrl: http://storage-be.storage.svc.cluster.local:80 



  replicaCount: 1

  image:
    repository: docker.io/vardhandevalla/dna-storage-frontend
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: fosstesting.oktav1


  imagePullSecrets:
    - name: vddockerhub

  nameOverride: ""
  fullnameOverride: "storage-mfe"

  podSecurityContext:
    runAsUser: 1001

  containerPort: 3000
  service:
    type: ClusterIP
    port: 80
  
  ingress:
    enabled: false
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: Prefix
    tls:
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
      - hosts:
        - chart-example.local
        secretName: my-tls-secret

storagebe:
  enabled: true
    # Default values for storage-be.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  namespace: storage
  #Add new env as array value with name/value field. Keep one tab indentation.
  envs:
    - name: MAX_FILE_SIZE
      value: 3000MB
    - name: MAX_REQUEST_SIZE
      value: 3000MB
    - name: VAULT_HOST
      value: vault.vault.svc.cluster.local
    - name: VAULT_PORT
      value: "8200"         
    - name: VAULT_SCHEME
      value: http
    - name: VAULT_AUTHENTICATION
      value: TOKEN
    - name: VAULT_TOKEN
      valueFrom:
        secretKeyRef:
          key: vaultToken
          name: storage-be
    - name: VAULT_MOUNTPATH
      value: kv
    - name: VAULT_PATH
      value: dna/minio  
    - name: DNA_URI
      value: http://dna-service.dna.svc.cluster.local:80
    - name: DNA_AUTH_ENABLE
      value: "true"  
    - name: JWT_SECRET_KEY
      valueFrom:
        secretKeyRef:
          key: jwt.secret.key
          name: storage-be
    - name: CORS_ORIGIN_URL
      value: http://* 
    - name: MINIO_ENDPOINT
      value: http://minio.storage.svc.cluster.local:9000
    - name: MINIO_CLIENT_API
      value: ""
    - name: MINIO_ADMIN_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          key: minioAccessKey
          name: storage-be  
    - name: MINIO_ADMIN_SECRET_KEY
      valueFrom:
        secretKeyRef:
          key: minioSecretKey
          name: storage-be   
    - name: MINIO_POLICY_VERSION
      value: "2012-10-17"   
    - name: LOGGING_ENVIRONMENT
      value: DEV   
    - name: LOGGING_PATH
      value: /tmp/storage/log/
    - name: ATTACHMENT_MALWARE_SCAN
      value: "true"  
    - name: MALWARE_SCANNER_URI
      value: http://localhost:9002/api/v1
    - name: MALWARE_SCANNER_APP_ID
      valueFrom:
        secretKeyRef:
          key: malwareApiId
          name: storage-be  
    - name: MALWARE_SCANNER_API_KEY
      valueFrom:
        secretKeyRef:
          key: malwareApiKey
          name: storage-be
    - name: API_DB_USER
      valueFrom:
        secretKeyRef:
          key: dbUser
          name: storage-be
    - name: API_DB_PASS
      valueFrom:
        secretKeyRef:
          key: dbPassword
          name: storage-be
    - name: API_DB_URL
      value: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/storage
    - name: NAAS_BROKER
      value: my-release-kafka-headless.kafka.svc.cluster.local:9092
    - name: STORAGE_TERMS_OF_USE_URL
      value: ""
    - name: DATAIKU_PROD_URI
      value: ""    
    - name: DATAIKU_PROD_API_KEY
      valueFrom:
        secretKeyRef:
          key: dataikuProdApiKey
          name: storage-be
    - name: DATAIKU_TRAINING_URI
      value: ""
    - name: DATAIKU_TRAINING_API_KEY
      valueFrom:
        secretKeyRef:
          key: dataikuTrainingApiKey
          name: storage-be  

  secret:
    - key: vaultToken
      value: hvs.3FpK0LNiEULFPuuyAOKlWJiv
    - key: jwt.secret.key 
      value: oeRaYY7Wo24sDqKSX3IM9ASGmdGPmkTd9jo1QTy4b7P9Ze5_9hKolVX8xNrQDcNRfVEdTZNOuOyqEGhXEbdJI-ZQ19k_o9MI0y3eZN2lp9jow55FfXMiINEdt1XR85VipRLSOkT6kSpzs2x-jbLDiz9iFVzkd81YKxMgPA7VfZeQUm4n-mOmnWMaVX30zGFU4L3oPBctYKkl4dYfqYWqRNfrgPJVi5DGFjywgxx0ASEiJHtV72paI3fDR2XwlSkyhhmY-ICjCRmsJN4fX1pdoL8a18-aQrvyu4j0Os6dVPYIoPvvY0SAZtWYKHfM15g7A3HD4cVREf9cUsprCRK93w
    - key: minioAccessKey 
      value: admin
    - key: minioSecretKey
      value: S6O!#uiOop
    - key: malwareApiId 
      value: cb86d44a-6886-46bd-a9a8-77a25d7da5f7
    - key: malwareApiKey
      value: 99759fb6-f7d0-4e97-94fb-21e61d5ec2cf
    - key: dbUser
      value: storage
    - key: dbPassword
      value: storage123
    - key: dataikuProdApiKey
      value: ""
    - key: dataikuTrainingApiKey
      value: ""  

  replicaCount: 1

  image:
    repository: docker.io/vardhandevalla/dna-storage-backend
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: fosstesting.oktav1

  imagePullSecrets:
    - name: vddockerhub
  nameOverride: ""
  fullnameOverride: "storage-be"

  podSecurityContext:
    #We are running this applicaton as root  
    runAsUser: 1001
    

  containerPort: 7175
  service:
    type: ClusterIP
    port: 80

  ingress:
    enabled: false
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
    hosts:
      - host: chart-example.local
        paths:
          - path: /storage/api
            pathType: Prefix
          - path: /storage/swagger-ui.html
            pathType: Prefix
          - path: /storage/swagger-resources
            pathType: Prefix
          - path: /storage/v2/api-docs
            pathType: Prefix
          - path: /storage/webjars
            pathType: Prefix
    tls:
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
      - hosts:
        - chart-example.local
        secretName: my-tls-secret

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
minio:
  enabled: true

#Subchart properties for model-registry

model-registry:
  enabled: false

trino-be:
  enabled: false