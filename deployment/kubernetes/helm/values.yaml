# Default values for helm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

#Suchart properties for frontend
frontend:
  enabled: true
  namespace: dna
  imagePullSecret:
    name: dockerregistry
    #Update the .dockerconfigjson value in the below key parameter.
    key: ""
  ingress:
    enabled: false
    namespace: ingress
    host: ""
    lbIP: ""
    
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: ""
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  configJs: |
      window["INJECTED_ENVIRONMENT"]={
        OIDC_DISABLED: true,
        OIDC_PROVIDER: "OKTA",
        API_BASEURL: "api",
        CLIENT_IDS: 'YOUR_CLIENT_ID',
        REDIRECT_URLS: 'YOUR_OKTA_REDIRECT_URL',
        OAUTH2_AUTH_URL: 'https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/authorize',
        OAUTH2_LOGOUT_URL: 'https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/logout',
        OAUTH2_REVOKE_URL: 'https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/revoke',
        OAUTH2_TOKEN_URL: 'https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/token',
        DNA_COMPANY_NAME: "Company_Name",
        DNA_APPNAME_HEADER: "DnA App",
        DNA_APPNAME_HOME: "Data and Analytics",
        DNA_CONTACTUS_HTML: "<div><p>There could be many places where you may need our help, and we are happy to support you. <br \/> Please add your communication channels links here<\/p><\/div>",
        DNA_BRAND_LOGO_URL: "/images/branding/logo-brand.png",
        DNA_APP_LOGO_URL: "/images/branding/logo-app.png",
        ENABLE_INTERNAL_USER_INFO: false,
        ENABLE_DATA_COMPLIANCE: false,
        ENABLE_JUPYTER_WORKSPACE: true,
        JUPYTER_NOTEBOOK_URL: "http://localhost:9001",
        JUPYTER_NOTEBOOK_OIDC_POPUP_URL: "http://localhost:9001/hub/oauth_login?next=",
        JUPYTER_NOTEBOOK_OIDC_POPUP_WAIT_TIME: 5000,
        ENABLE_DATAIKU_WORKSPACE: false,
        DATAIKU_LIVE_APP_URL: "",
        DATAIKU_TRAINING_APP_URL: "",
        DATAIKU_FERRET_URL: "",
        ENABLE_MALWARE_SCAN_SERVICE: true,
        MALWARE_SCAN_SWAGGER_UI_URL: "http://localhost:9002/swagger-ui.html",
        ENABLE_MALWARE_SCAN_ONEAPI_INFO: false,
        ENABLE_DATA_PIPELINE_SERVICE: true,
        ENABLE_STORAGE_SERVICE: true,
        STORAGE_MFE_APP_URL: "http://localhost:7175",
        ENABLE_REPORTS: true,
        ENABLE_ML_PIPELINE_SERVICE: false,
        ENABLE_NOTIFICATION: true,
        DATA_PIPELINES_APP_BASEURL: "http://localhost:9010",
        DATA_PIPELINES_API_BASEURL: "http://localhost:9003/airflow/api",
        NOTIFICATIONS_API_BASEURL: "http://localhost:9004/api",
        DASHBOARD_API_BASEURL: "http://localhost:9005/api",
        MALWARESCAN_API_BASEURL: "http://localhost:9002/api",
        ML_PIPELINE_URL: "",
        MODEL_REGISTRY_API_BASEURL: "",
        INTERNAL_USER_TEAMS_INFO: '(Recommended to use Short ID. To find Short ID use <a href=\"YOUR_TEAMS_INFO_URL\" target=\"_blank\" rel=\"noreferrer noopener\">Teams<\/a>)'
      };

      window["STORAGE_INJECTED_ENVIRONMENT"]={
        CONTAINER_APP_URL: "http://localhost:8080",
        STORAGE_API_BASEURL: "http://localhost:7175/api",
        TOU_HTML: "<div>I agree to <a href=\"#\" target=\"_blank\" rel=\"noopener noreferrer\">terms of use<\/a><\/div>",
        ENABLE_DATA_CLASSIFICATION_SECRET: false,
        API_BASEURL: "http://localhost:8080/api"
      };
  
  probes:
    initialDelaySeconds: 30
    timeoutSeconds: 10
    periodSeconds: 10
    failureThreshold: 3

    readinessProbe:
      path: /
      port: 3000
      
  appFrontend:
    replicaCount: 1
  #Build & update the image of the app-frontend service in the docker-compose-local-basic and update the value here.
    image: dna/app-frontend
    ngnix:
      backend: http://dna-service.dna.svc.cluster.local:80
      grafanaServer: http://i3-monitoring-grafana.i3-monitoring.svc.cluster.local:80
      avscanMgwServer: http://dna-microgateway.clamav.svc.cluster.local:80

#Suchart properties for backend
backend:
  enabled: true
  namespace: dna
  imagePullSecret: 
    name: dockerregistry
  #Update the .dockerconfigjson value in the below key parameter.
    key:  
  ingress:
    enabled: false
    namespace: ingress
    host: ""
    lbIP: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  app:
    backend:
      replicaCount: 1
      #Update the image of the app-backend service in the docker-compose-local-basic.yml
      image: dna/app-backend
      #Specify the secrets which will be used by backend to run properly, Secrets will be encoded in base64 at the time of deployment
      secrets: 
        name: app-secrets
        #Notebook secret key was the jupyter.api_token that was generated in the hub-config.yaml
        notebookSecretToken: ""
        oidcClientID: YOUR_CLIENT_ID
        oidcClientSecret: YOUR_CLIENT_SECRET
        drdCertPassword: ""
        jwtSecretKey: ""
        s3AccessKey: admin
        #Generate the minio secret key as per the documentation and update this below paramter "s3Secretkey"
        s3SecretKey: 
        dataikuProdApiKey: ""
        dataikuTrainingApiKey: "" 
        #Go to Malware scan and generate the api key . After generating copy the application key in the below parameter "avscanApikey"
        avscanApiKey: ""
        appDBUserName: postgres
        appDBPassword: postgres
      config:
        enableItsmm: false
        enableJupyterNotebook: true
        enableDataiku: false
        enableAttachmentScan: true
        enableInternalUser: false
        redirectUrl: http://localhost:8080
        dbUri: jdbc:postgresql://dna-bitnamipostgresql:5432/db
        oidcUserInfoUrl: https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/userinfo
        oidcTokenIntrospectionUrl: https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/introspect
        oidcProvider: OKTA
        oidcTokenRevocationUrl: https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/revoke
        internalUserRequestUrl: ""
        internalCertFile: ""
        oidcDisabled: true
        s3BucketName: dna
        s3Url: http://minio.storage.svc.cluster.local:9000
        corosOriginUrl: http://*
        jupyterNotebookUrl: http://proxy-public.notebooks.svc.cluster.local:8000/hub/api/users
        dataikuProdUri: ""
        dataikuProdAdminGroup: "" 
        dataikutraininguri: ""
        #update the application id in the avscanAppID parameter from the generated api key
        avscanAppId: ""
        naasBroker: ""
        loggingEnvironment: dev
        loggingPath: /tmp/app/log
        dataikuProjectUri: /projects/
	#The below feature like apacCorpdir,emeaCorpDir,dataikuUserRoleUriPath,PermissionUriPath are coming soon features
        apacCorpDir: ""
        emeaCorpDir: ""
        dataikuUserRoleUriPath: ""
        dataikuPermissionUriPath:
        dataikuTrainingAdminGroup: ""
        avscanUri: http://clamav-rest-service.clamav.svc.cluster.local:8181
        flywayBaseline: "true"
        flywayBaselineOnMigrate: "true"
        flywayBaselineVersion: "0"
        flywaySchema: "public"
        dashboardUri: http://dashboard-backend-service.dashboard.svc.cluster.local:7173

  probes:
    initialDelaySeconds: 120
    timeoutSeconds: 30
    periodSeconds: 10
    failureThreshold: 3
    livenessProbe:
      path: /actuator/health/liveness
      port: 7171
    readinessProbe:
      path: /actuator/health/readiness
      port: 7171 

#Subchart properties for i3postgressql database
postgresql-helm:
  enabled: false
  namespace: dna
  ##
  image:
    repository: postgresql
    tag: latest
    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent

  ## PostgreSQL replica count
  ##
  replicaCount: 3

  printEnv: false

  ## PostgreSQL connection settings - max connections
  ## ref: https://www.postgresql.org/docs/current/runtime-config-connection.html#GUC-MAX-CONNECTIONS
  ##
  pgconnectionconfig:
    enabled: false
    maxconnections: 100

  ## PostgreSQL memory resource limits - shared buffers
  ## ref: https://www.postgresql.org/docs/current/runtime-config-resource.html#GUC-SHARED-BUFFERS
  ##
  pgmemoryresources:
    enabled: false
    sharedbuffers: 128MB

  ## PostgreSQL parameter wal_level. Default wal_level is 'replica'. Use wal_level 'logical' with Debezium and Kafka.
  ## ref: https://www.postgresql.org/docs/12/runtime-config-wal.html
  ##
  pg_wal_level: replica
  #pg_wal_level: logical

  ## Set to true to enable the pgAudit extension before initializing the DB
  ## By default the pgAudit extension is inactive. Each application owner needs to make sure to be compliant with GDPR (DSGVO) before enabling the PGAudit feature.
  ## Configure the pgAudit extension with 'parameters' below. ref: https://github.com/pgaudit/pgaudit#settings
  ##
  pgaudit:
    enabled: false

  ## PostgreSQL config parameter map.
  ## ref: https://www.postgresql.org/docs/current/config-setting.html
  ##
  parameters:
    max_wal_senders: 10
    work_mem: 4MB
  #  pgaudit.log: 'write,ddl'

  ## Set to true if you would enable SSL
  ##
  SSL:
    enabled: true
    ## uncomment and set values when you bring yor own SSL certificate and key
    # tlscrt: <Base64 encoded certificate>
    # tlskey: <Base64 encoded key>

  ## uncomment when your database client cannot authenticate with "SCRAM-SHA-256" by default
  # PGPWENCRYPT: "MD5"

  ## more optional PostgreSQL configuration parameters. See docs/configuration/configuration-database.md for details.
  #
  # PGDBCREATEPARAMS: "ENCODING 'LATIN1'"
  # PGNETWORK: "0.0.0.0/0"
  # PGBACKUPHOST: "0.0.0.0/0"
  # PGHARDPROP: ""
  # PATRONIADDRESS: "0.0.0.0/0"
  # PGENABLECSVLOG: "on"
  # PGSYNCCOMMIT: "on"
  # PGSYNCSTANDBYNAMES: "*"
  # RECOVERY_TIMEOUT: "300"

  ## Specify PGDATABASE
  ##
  DBName: db

  ## Configure S3 properties if S3 storage will be used
  ##
  s3:
    ## provided by s3 provider
    accesskey: ""
    ## provided by s3 provider
    secretkey: ""
    ## internally used
    hostAlias: s3
    ## url of the S3 endpoint
    host: ""
    ## use rclone client in insecure mode
    ## in some cases, e.g. if you want to test with your own S3 service hosted on your own hardware, the certificate might not be verifiable.
    insecure: false
    ## rclone client option for debug output to console
    debug: false

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    enabled: false
    ## uncomment fitting parts when you wand to enable setting cpu and memory requests and limits
    limits:
      enabled: false
      # cpu: 1.5
      # memory: 1Gi
    request:
      enabled: false
      # cpu: 1.2
      # memory: 1Gi

  ## Configure additional env variables
  env:
  #- name: USERPW
  #  valueFrom:
  #    secretKeyRef:
  #      name: my-postgres-postgresql-helm
  #      key: postgres.user.password

  ## Configure WAL archiving
  ## for point-in-time recovery WAL archiving must be enabled
  ## for details on WAL archiving see https://www.postgresql.org/docs/10/continuous-archiving.html
  walArchiving:
    enabled: false
    # you have to choose the type of storage where the WAL files will be archived
    # fs | s3
    storage: fs
    fs:
      ## Configuration for FS
      targetdir: /tmp/wal_archiving
      volume:
        ## if volume.enabled is true, a PersistentVolumeClaim will be created
        ## if it is false, the WAL files will be archived to an emptyDir (makes only sense for testing)
        enabled: false
        accessModes:
        - ReadWriteMany
        size: 1Gi
        storageClass: dhc-backup-nfs
    s3:
      ## Configuration for S3 storage
      subfolder: postgresql-cluster
      ## fill in the bucket name you have ordered!
      bucketName: emeas3

  ## Configure extra options for liveness and readiness probes
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes)
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 5
    timeoutSeconds: 1
    failureThreshold: 3
    successThreshold: 1

  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30
    successThreshold: 1

  ## Security context for mounted volumes and containers
  ##
  ## Pod Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  ##
  ## Use setgid: true and setuid: true on DHC CaaS and Kubernetes platform
  ## Use setgid: false and setuid: false on OpenShift platforms
  securityContext:
    setgid: true
    setuid: true
    runAsUser: 987
    fsGroup: 987
    runAsGroup: 987
    fsGroupChangePolicy: "OnRootMismatch"

  ## Set to false if you want to deploy credentials not within secret files
  ##
  secrets:
    enabled: true
    # Set this to the name of the secret holding sensitive data. It must be created before PODs and must
    # contain all configured key/value pairs. If not set, and enabled is set to true, a secret will be
    # created from the data specified down below.
    #externalSecretName: mySecret
    # Set this to the name of the secret holding the decryption private key and its passphrase. It must
    # be created before restore-job is ran. If not set, and enabled is set to true, a secret will be
    # created from the data specified in the 'encryptBackupWal' snippet down below.
    #externalGpgSecretName: mySecret-gpg-private

  ## PostgreSQL database users
  ## It is recommended to change the dbusers values to your own!
  ##
  dbusers:
    ## backup user, will be used for synchronizing between the cluster nodes and for backup
    backup:
      username: ""
      password: ""
    ## the database (not the instance) owner
    dbadm:
      username: ""
      password: ""
      createSchema:
        # Usually this is required if you use an ORM and do not create the table via the initdb script
        enabled: false
        # name: customschema
    ## technical user for postgresql monitoring
    pgmon:
      username: ""
      password: ""
    ## if a technical user for Debezium is needed, then set a password
    ## otherwise no user for Debezium is created
    debezium:
      username: ""
      password:
    ## if technical user 'DATA_CATALOG' for CORPORATE DATA CATALOG is needed, then set a password
    ## otherwise no user for CORPORATE DATA CATALOG is created
    dataCatalog:
      password:

  ## Enable PostgreSQL service
  ##
  service:
    enabled: true
    ## Attention!
    ## The service name has to be unique inside a namespace!
    ## Otherwise there will be a clash of endpoint and the service will not work!
    # name: postgres
    port: 64000

  ## PostgreSQL persistence layer
  ## storageClassName: <storageClass>
  ##
  persistence:
    enabled: true
    size: 1Gi
    # storageClass: cinder
    # PVCName: name
    wal:
      enabled: false
      size: 1Gi
      # storageClass: cinder
      # PVCName: name

  ## Network Policy to be applied
  ## By default an ingress rule which allows the communication between all pods of this application, is already defined in templates/networkpolicies.yaml.
  ## Additionally a namespaceSelector can be added
  ##
  networkPolicy:
    enabled: true
    ## here you can define additional rules, e.g.:
    #ingress:
    #- from:
    #  - podSelector:
    #      matchLabels:
    #        app: db

  ## encryption of backup and WAL files
  ##
  encryptBackupWal:
    enabled: false
    publicKey: # <Base64 encoded key>
    keyId: # keyId or fingerprint

  ## decryption of backup and WAL files
  ##
  decryptBackupWal:
    enabled: false
    passphrase: # passphrase
    privateKey: # <Base64 encoded key>

  ## PostgreSQL backup configuration
  ##
  backup:
    enabled: false

    ## the storage type, either fs or s3.
    ## This is the target storage where the compressed backup archive file will be stored.
    storage: fs
    ## Configuration for FS
    fs:
      volume:
        ## if true, an PersistentVolumeClaim will be created
        ## else the backup will be written to an `emptyDir`
        enabled: false
        accessModes:
        - ReadWriteMany
        size: 1Gi
        storageClass: dhc-backup-nfs
    s3:
      ## Configuration for S3 storage
      # subfolder: postgresql-cluster
      ## fill in the bucket name you have ordered!
      bucketName: emeas3
    ## general configuration for backup
    schedule: "0 0 * * *"
    retention: 5
    ## Please read documentation at docs/operations/operations.md#cleanup-wal-archives before enabling this feature
    cleanupWalArchives: false
    image:
      repository: postgresql-backup
      tag: 13.6-5.1.1-2022.6.15
      pullPolicy: IfNotPresent
    resources:
      enabled: false
      limits:
        enabled: false
        # cpu: 500m
        # memory: 512Mi
      request:
        enabled: false
        # cpu: 250m
        # memory: 256Mi

    ## this volume is a temporary used volume where the backup container stores the copy of the master.
    ## for small databases its ok to set this to false, the required space is equals to the used space of the master node.
    tempPgdataVolume:
      ## recommended
      enabled: true
      size: 1Gi
      # storageClass: cinder
    file:
      name: postgresql-backup
      # label: test

  ## PostgreSQL restore configuration
  ##
  restore:
    ## whether to create the restore job or not
    createRestoreJob: false
    ## whether to create the list backup job or not
    createListBackupJob: false
    ## if neither targetTime nor targetTimeline has been set, the recovery_target is 'immediate' which means that there will be no point-in-time-recovery (PITR) done
    ## the format is YYYY-MM-DD HH:mm:ss, e.g. 2020-12-31 23:59:59
    #targetTime: 2020-12-31 23:59:59
    # "latest" recovers to the latest timeline (poss. over different timelines) found in the WAL archive
    targetTimeline: latest
    ## "true" means that given recovery target time is inclusive, applies when recovery_target_time is set
    targetInclusive: true
    ## the kind of storage where the backups are stored
    ## fs | s3
    storage: fs
    fs:
      volume:
        ## if volume.enabled is true, it is expected that a PersistentVolumeClaims exists with the backups and that it can be mounted.
        enabled: true
    s3:
      # subfolder: postgresql-cluster
      ## fill in the bucket name you have ordered!
      bucketName: emeas3

    ## Complete (including label) file name of backup file that has to be restored.
    recoveryFile: postgresql-backup*.tar.gz
    image:
      repository: postgresql-backup
      tag: 13.6-5.1.1-2022.6.15
      pullPolicy: IfNotPresent
    resources:
      enabled: false
      limits:
        enabled: false
        # cpu: 500m
        # memory: 512Mi
      request:
        enabled: false
        # cpu: 250m
        # memory: 256Mi

    ## false for dry-run
    targetVolume:
      enabled: true

  ## initdb scripts
  ## Specify a script to be run at first boot
  ##
  initdbScripts:
    enabled: false
    name: init_db.sh
    script: |-
      ##!/bin/bash

      # PREREQUISITE: EXISTING ENV variable 'USERPW' with your required db user password. For more information please refer to our documentation.

      #psql <<INITSQL
      #    CREATE ROLE i3user LOGIN ENCRYPTED PASSWORD '${USERPW}';
      #    GRANT dai_db_connect to i3user;
      #    GRANT dai_public_compat to i3user;
      #    CREATE SCHEMA IF NOT EXISTS i3 AUTHORIZATION i3user;
      #    ALTER ROLE i3user SET search_path to i3;
      #INITSQL

      #psql -U i3user <<INITSQL
      #    CREATE TABLE person(
      #      id serial PRIMARY KEY,
      #      username VARCHAR (50) UNIQUE NOT NULL,
      #      email VARCHAR (50)  UNIQUE NOT NULL
      #    );
      #INITSQL

  test:
    enabled: true

  serviceAccount:
    enabled: true

  endpoints:
    enabled: true

  bypassApiService:
    enabled: false

  createGlobalResources: false

  statefulSet:
    enabled: true

  pgmonitoring:
    enabled: true
    image:
      repository: postgres-exporter
      tag: v0.10.1
      pullPolicy: IfNotPresent
    resources:
      enabled: false
      limits:
        enabled: false
        # cpu: 500m
        # memory: 512Mi
      request:
        enabled: false
        # cpu: 250m
        # memory: 256Mi
    queryMonitoring:
      enabled: true
    customQueries: {}
    #  pg_replication:
    #    query: "SELECT CASE WHEN pg_last_wal_receive_lsn() = pg_last_wal_replay_lsn() THEN 0 ELSE EXTRACT (EPOCH FROM now() - pg_last_xact_replay_timestamp()) END as lag"
    #    metrics:
    #      - lag:
    #          usage: "GAUGE"
    #          description: "Replication lag behind master in seconds"

    dashboard:
      # If you have a i3-monitoring installation running, you can enable here the deployment of a grafana dashboard
      # to i3-monitoring via configmap
      deploy: false
      file:
      - files/i3-postgres-exporter-dashboard.json
      - files/i3-postgres-query-exporter-dashboard.json
      i3-monitoring-namespace: i3-monitoring

  ## Manually specify POD affinity. Overrides antiAffinity set down below.
  affinity: {}
    #  # schedule all instances from the same release on a different node.
    #  podAntiAffinity:
    #    requiredDuringSchedulingIgnoredDuringExecution:
    #      - labelSelector:
    #          matchExpressions:
    #            - key: app.kubernetes.io/component
    #              operator: In
    #              values:
    #                - "postgresql-node"
    #            - key: app.kubernetes.io/name
    #              operator: In
    #              values:
    #                - postgresql-helm
    #            - key: app.kubernetes.io/instance
    #              operator: In
    #              values:
    #                - mypg-instance
    #        topologyKey: "kubernetes.io/hostname"

  antiAffinity:
    enabled: true

  podDisruptionBudget:
    maxUnavailable: 1

  terminationGracePeriodSeconds: 300

  mountPGLibs:
    enabled: false
    pgVersion: "13.6"
    image:
      repository: postgresql
      tag: 13.6-5.1.1-2022.6.15
      pullPolicy: IfNotPresent
    resources:
      enabled: false
      limits:
        enabled: false
        # cpu: 500m
        # memory: 512Mi
      request:
        enabled: false
        # cpu: 250m
        # memory: 256Mi

  ## Start master and slave(s) pod(s) without limitations on shm memory.
  ## By default docker and containerd (and possibly other container runtimes)
  ## limit `/dev/shm` to `64M` (see e.g. the
  ## [docker issue](https://github.com/docker-library/postgres/issues/416) and the
  ## [containerd issue](https://github.com/containerd/containerd/issues/3654),
  ## which could be not enough if PostgreSQL uses parallel workers heavily.
  ##
  shmVolume:
    enabled: false

  ## Load balancer config for CORPORATE DATA CATALOG
  ## for details on CORPORATE DATA CATALOG integration see our [documentation](../../docs/operations/operations.md#corporate-data-catalog-integration)
  lb:
    enabled: false
    # ip:
    port: 4400

  ## define permanent replication slots. These slots will be preserved during switchover/failover
  slots:

#Subchart properties for bitnami postgresql
bitnamipostgresql:
  enabled: true
  namespace: dna
  global:
    postgresql:
      postgresqlDatabase: db
      postgresqlUsername: postgres
      postgresqlPassword: postgres
      servicePort: 5432

#Subchart properties for clamav
clamav:
  enabled: true
  appName: clamav
  namespace: clamav
    
  securityContext:
    runasUser: 0 
  app:
    backend:
      name: clamav-rest
      #Update the image of the dna/malware-backend service in the docker-compose-local-basic.yml
      image: dna/malware-backend
      replicaCount: 1
      secrets:
        name: oneapi-secrets
        onapiBasicAuthToken: ""
        jwtSecretKey: ""
        appDBUserName: malware
        appDBPassword: malware123
      config: 
        clamav_backend_url: clamav-service
        clamav_backend_port: 3310
        max_file_size: 4000MB
        max_request_size: 4000MB
        api_request_limit: 20
        with_in: 2
        time_unit: seconds
        auth_api_host: http://dna-service.dna.svc.cluster.local:80/api/subscription
        restricted_url_pattern: api/v1/*
        loggingPath: /tmp/clamav/log
        loggingEnvironment: dev
        corsOriginUrl: http://*
        dbUri: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/malware
        flywayEnabled: true
        flywayBaselineOnMigrate: true
        flywayBaselineVersion: 0
        flywaySchema: public
        vaultHost: vault.vault.svc.cluster.local
        vaultPort: 8200
        appUrl: http://dna-service.dna.svc.cluster.local:80
        enableAuth: true
        verifyLoginApi: /api/verifyLogin
        unscribeMalwareScanApi: /api/malwarescan/unsubscribe/
        naas_broker: ""

  #Update your kafka service FQDN name in the "NAAS_BROKER" parameter
        naas_broker: ""
  #The root token that will be generated while initalizing the vault service
    vault:
      secret:
        rootToken: ""

    probes:
      initialDelaySeconds: 60
      timeoutSeconds: 10
      periodSeconds: 10
      failureThreshold: 3
      livenessProbe:
        path: /actuator/health/liveness
        port: api
      
      readinessProbe:
        path: /actuator/health/readiness
        port: api

  #Update the image of clamav service in the docker-compose-local-basic.yml
  image:
    repo: mkodockx/docker-clamav:stretch-slim
    replicaCount: 1
    pullPolicy: Always
  ingress:
    enabled: false
    host: "" 
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: "" 
  imagePullSecret: 
    name: dockerregistry
    key: "docker-config" 

  Storage:
    storageClass: ""
    accessModes: ReadWriteOnce
    size: 2G

  resources:
    requests:
      memory: "5000Mi"
      cpu: "500m"
    limits:
      memory: "8000Mi"
      cpu: "700m"
      
#Subchart properties for naas
naas:
  enabled: true
  appName: naas
  namespace: naas
  #update the image name of the naas-backend service in the docker-compose-local-basic.yml
  app:
    backend:
      image: dna/naas-backend
      secrets:
        name: naas-app-secrets
        authApiToken: ""
        jwtKey: 
        db: 
          appUserName: postgres
          appPassword: postgres
      config:
        api_db_url: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/db
        naas_broker: ""
        max_poll_records: 6000
        dna_uri: http://dna-service.dna.svc.cluster.local:80
        dna_auth_enable: true
        mailServerHost: ""
        mailServerPort: ""
        notificationSenderEmail: "" 
        poll_time: 5000
        naas_central_topic: dnaCentralEventTopic
        naas_centralread_topic: dnaCentralReadTopic
        naas_centraldelte_topic: dnaCentralDeleteTopic
        loggingPath: /tmp/naas/log
        loggingEnvironment: dev
        corsOriginUrl: http://*
    # resources:
    #   requests:
    #     memory: "512Mi"
    #     cpu: "250m"
    #   limits:
    #     memory: "1000Mi"
    #     cpu: "500m"
    probes:
      initialDelaySeconds: 180
      timeoutSeconds: 10
      periodSeconds: 30
      failureThreshold: 3
      livenessProbe:
        path: /actuator/health/liveness
        port: api
      
      readinessProbe:
        path: /actuator/health/readiness
        port: api
  
  image:
    pullPolicy: Always
      
  ingress:
    enabled: false
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  imagePullSecret: 
   name: dockerregistry
   key: "docker-config" 
  
#Subchart properties for notebooks
notebooks:
  enabled: true 
  namespace: notebooks
  app:
    jupyter:
      config:
        configproxy_auth_token: ""
        kf_pipelines_endpoint: ""
  #Build & update the image of the result hub service in the docker-compose-notebook.yml 
    image:
      name: jupyterhub:latest
      pullPolicy: Always

  #Build & Update the images of the pyspark-notebook-default & pyspark-notebook-tensorflow services in the docker-compose-notebook.yml . Keep the chronos parameter to empty value
    profileListImages:
      default: pyspark-notebook-default:latest
      tensorflow: pyspark-notebook-tensorflow:latest
  #Chronos image is a coming soon feature
      chronos: ""

  #Build & update the image of the result-configurable-httpproxy services in the docker-compose-notbook.yml
    proxy:
      image: configurable-http-proxy:latest

  #Build & update the image of the pyspark-notebook-base service in the docker-compose-notebook.yml
    hubConfig:
      name: hub-config
      KubeSpawnerimage: pyspark-notebook-default:latest
      serviceAccount: hub 
      #Inorder to use the iframe for the notebooks service in the dna platform update the security context with frontend domain & Notebook domain name . For testing purpose we port-forwarded the dna-frontend and notebook service to 8080 and 9001 respectively and mentioned in the security context
      securitycontext: HOST http://localhost:8080/* http://localhost:9001
      oauthAuthenticator: GenericOAuthenticator
      oauthClientId: YOUR_CLIENT_ID
      oauthClientSecret: YOUR_CLIENT_SECRET
      oauthCallback: http://localhost:9001/hub/oauth_callback
      oauthAuthorizeUrl: https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/authorize
      oauthTokenUrl: https://YOUR_OKTA_DOMAIN.okta.com/v1/token
      oauthUserDataUrl: https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/userinfo
      oauthUsrKey: sub
      #OIDC Provider = oauthLoginSvc
      oauthLoginSvc: OKTA
      prespawn_hook: ""
      enableUserNS: "False"
      userNameSpaceTemplate: kubeflow

  ingress:
    enabled: false
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""

  Storage:
    storageClass: ""
    accessModes: ReadWriteOnce
    size: 1G

  resources:
    cpu: 1
    memory: 4G
  
  # resources:
  #   cpu: 1
  #   memory: 4G

#Subchart properties for dashboard
dashboard:
  enabled: true
  namespace: dashboard
  #Build & Update the image of the dashboard-backend in the docker-compose-local-basic.yml
  image: dna/dashboard-backend
  imagePullSecret: 
    name: dockerregistry
    key: "docker-config" 
  dbUrl: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/dashboard
  secret:
    name: dashboard-secrets
    appUserName: dashboard
    appPassword: dashboard
    jwtKey: ""
  appUrl: http://dna-service.dna.svc.cluster.local:80
  enableAuth: true
  loggingPath: /tmp/dashboard/log
  loggingEnvironment: dev
  flywayBaseline: "true"
  flywayBaselineOnMigrate: "true"
  flywayBaselineVersion: "0"
  flywaySchema: "public"
  containerPort: 7173
  corsOriginUrl: http://*

  #Update your kafka service FQDN name in the naasBroker parameter
  naasBroker: ""

  ingress:
    enabled: false
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1000Mi"
      cpu: "500m"

  probes:
    initialDelaySeconds: 120
    timeoutSeconds: 10
    periodSeconds: 20
    failureThreshold: 3
    livenessProbe:
      path: /actuator/health/liveness
      port: api
        
    readinessProbe:
      path: /actuator/health/readiness
      port: api

#Subchart properties for airflow
airflow:
  enabled: true
  appName: airflow
  namespace: airflow
   imagePullSecret: 
    name: dockerregistry
    #update the .dockerconfigjson of our image registry into the below key parameter
    key:  ""
  #Update the Airflow backend image name to the below parameter (docker-compose -f docker-compose-airflow.yml build)
  backend:
    image: airflow_backend
    imagePullPolicy: Always
  #Update the jdbc postgresql url to the below url paramater "jdbc:postgresql://host(Datbase service FQDN):port/database"
    dbUrl: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/airflow
    secret:
      name: airflow-backend-secrets
  #Update your airflow database username and passwords to the below values 
      dbPassword: airflow
      dbUsername: airflow
      gitToken: ""
      jwtKey: ""
      oidcClientID: YOUR_CLIENT_ID 
      oidcClientSecret: YOUR_CLIENT_ID
    containerPort: 7171
    crossOriginUrl: http://*
    apiUrl: http://dna-service.dna.svc.cluster.local:80
    oidcInfoUrl: https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/userinfo  
    oidcIntrospectionUrl: https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/introspect 
    oidcRevocationUrl: https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/revoke  
    oidcDisabled: true
    gitUrl: ""
    gitMountPath: /git/airflow-user-dags
    gitBranch: main
    dag:
      path: dags
      ext: py
      waitTime: 20
      retry: 20
    loggingPath: /tmp/airflow/log
    loggingEnvironment: dev
    flywayBaseline: "true"
    flywayBaselineOnMigrate: "true"
    flywayBaselineVersion: "0"
    flywaySchema: "public"
  ##Ingress for airflow backend service
  ingress:
    enabled: false
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: ""
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  ##Ingress for the airflow Application
  airflow:
    ingress:
      enabled: false
      host: ""
      annotations:
        traefik.frontend.rule.type: PathPrefix
        kubernetes.io/ingress.class: ""
        traefik.ingress.kubernetes.io/router.tls: "true"
        traefik.ingress.kubernetes.io/router.entrypoints: websecure
        cert-manager.io/cluster-issuer: ""
  #pullsecretdata was nothing but the key that was used to pull the images from the registry
  pullSecretData: ""
  secret:
    gitUserName: ""
    gitPassword: ""
    #gitSshKey: ""
    knownHosts: ""
    postgresql:
    #sqlAlchemyconn == postgresql+psycopg2://<database_username>:<database_password>@db_service_FQDN:DB_port/<database_name>
      sqlAlchemyConn: postgresql+psycopg2://airflow:airflow@dna-bitnamipostgresql.dna.svc.cluster.local:5432/airflow
    clientSecret: CSAgewogICAgIndlYiI6IHsKICAgICAgICAiY2xpZW50X2lkIjogIjBvYTVoczQxNXRqeDNveW9LNWQ3IiwKICAgICAgICAiY2xpZW50X3NlY3JldCI6ICJpVEVXVll2WXpwOXYxRkVHa3hKOTFOanR4ZmFZTDZlRUMteDZOT0Y5IiwKICAgICAgICAiYXV0aF91cmkiOiAiaHR0cHM6Ly9kZXYtMzY5ODA1OTUub2t0YS5jb20vb2F1dGgyL3YxL2F1dGhvcml6ZSIsCiAgICAgICAgInRva2VuX3VyaSI6ICJodHRwczovL2Rldi0zNjk4MDU5NS5va3RhLmNvbS9vYXV0aDIvdjEvdG9rZW4iLAogICAgICAgICJ1c2VyaW5mb191cmkiOiAiaHR0cHM6Ly9kZXYtMzY5ODA1OTUub2t0YS5jb20vb2F1dGgyL3YxL3VzZXJpbmZvIiwKICAgICAgICAiaXNzdWVyIjogImh0dHBzOi8vZGV2LTM2OTgwNTk1Lm9rdGEuY29tIiwKICAgICAgICAicmVkaXJlY3RfdXJpcyI6IFsiaHR0cDovL2xvY2FsaG9zdDo5MDEwLyoiXQogICAgfQp9
    #clientsecret is a base64 encoded value of the below OIDC parameters
    # {
    # "web": {
    #     "client_id": YOUR_CLIENT_ID,
    #     "client_secret": YOUR_CLIENT_ID,
    #     "auth_uri": "https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/authorize",
    #     "token_uri": "https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/token",
    #     "userinfo_uri": "https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/userinfo",
    #     "issuer": "https://YOUR_OKTA_DOMAIN.okta.com",
    #     "redirect_uris": ["http://localhost:9010/*"]
    # }
    #   }
  #update the git-sync image to the below image paramaeter (docker build ./dockerfiles/airflow -t <image_name_of_your_wish> -f ./dockerfiles/airflow/git-sync.Dockerfile)
  gitSync:
    image: dna/git-sync:latest
    repo: 
    dest: git-sync
    branch: main
    ssh: "false"
    root: /git

  airflowIngressRoute:
    enabled: false 
  airflowMiddleware: 
    enabled: false
  # editor:
  #   gitEnabled: false
  #   gitCMD: /usr/bin/git
  #   gitDefaultArgs: -c color.ui=true
  #   gitIntRepo: False
  #   lineLength: 88
  #   stringNormalization: False
    
  configuration:
    loggingLevel: INFO
    executor: KubernetesExecutor
    parallelism: 32
    pluginsFolder: /usr/local/airflow/plugins
    loadExamples: False
    scheduler:
      dagDirListInterval: 5
      childProcessLogDirectory: /usr/local/airflow/logs/scheduler
      jobHeartbeatSec: 5
      parsingProcesses: 2
      schedulerHeartbeatSec: 5
      minFileProcessInterval: 0
      statsdOn: False
      statsdHost: localhost
      statsdPort: 8125
      statsdPrefix: airflow
      minFileParsingLoopTime: 1
      printStatsInterval: 30
      schedulerZombieTaskThreshold: 300
      maxTisPerQuery: 0
      authenticate: False
      catchupByDefault: True

  #Domain name of the airflow service need to used in the baseurl, For local testing purpose i am port-forwarding the airflow service to 9010
    webserver:
      baseUrl: http://localhost:9010
      path: /
      rbac: True
      host: 0.0.0.0
      port: 8080
      masterTimeout: 120
      workerTimeout: 120
      workerRefreshBatchSize: 1
      workerRefreshInterval: 30
  #This is secret key used to run the flask app and this can be generated by using the "Python command shown in the Flask docs: python -c 'import os; print(os.urandom(16))'Or, since the secret_key may be in unicode, you can use any online key generator to create one, or just enter some random string of letters and numbers"
      secretKey: ""
      numberOfWorkers: 4
      workerClass: sync
      exposeConfig: True
      dagDefaultView: graph
      dagOrientation: LR
      demoMode: False
      logFetchTimeoutSec: 5
      hidePausedDagsByDefault: False
      pageSize: 100
    kubernetes:
      workerContainerImagePullPolicy: Always
      workerServiceAccountName: airflow
      deleteWorkerPods: True
      dagsInImage: false
      gitSubpath: dags
      inCluster: True
      gitSyncContainerRepository: airflow-git-sync
      gitSyncContainerTag: latest
      gitSyncInitContainerName: git-sync-container
      gitSyncRunAsUser: 1000
      runAsUser: 1000
      fsGroup: 65533
    kubernetesLabels:
      airflowWorker:

  docker:
  #Build the airflow image using the airflow.Dockerfile
    image:
      name: dna/airflow-service
      tag: latest

  service:
    port: 8080

  db:
    port: 5432

  webserver:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 2000Mi
      cpu: 1000m

  scheduler:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 2000Mi
      cpu: 1000m

  gitContainer:
    requests:
      memory: 250Mi
      cpu: 250m
    limits:
      memory: 1000Mi
      cpu: 500m

  backendResources:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 1000Mi
      cpu: 500m

  volumes:
    logsPath: /usr/local/airflow/logs
    dagsPath: /usr/local/airflow/dags/git-sync/dags
    gitDagsPath: /usr/local/airflow/dags
    airflowLogsClaim:
      resourcePolicy: keep
      accessMode: ReadWriteOnce
      storage:
        className: ""
        size: 2Gi

  oidc:
    logout:
      uri: https://YOUR_OKTA_DOMAIN.okta.com/oauth2/v1/logout

  Storage:
    storageClass: ""
    accessModes: ReadWriteOnce
    size: 1Gi
  probes:
    initialDelaySeconds: 120
    timeoutSeconds: 10
    periodSeconds: 10
    failureThreshold: 3
    livenessProbe:
      path: /airflow/actuator/health/liveness
      port: api-http
      
    readinessProbe:
      path: /airflow/actuator/health/readiness
      port: api-http
  
#Subchart properties for microgateway

microgateway:
  enabled: false
  namespace: clamav
  proxy: ""
  noProxy: ""
  apigee:
    # environment variables 
    debug: "*" # Enable DEBUG mode with "*"
    key: ""
    secret: ""
    org: internal
    env: development
    # if introspection is required add the introspection credentials
    #introspection_client_id: 
    #introspection_client_secret: 
    
    # validate target https certificates (1=enabled; 0=disabled)
    node_tls_reject_unauthorized: 1

    certs:
      # key and cert will be mount under /home/node/certs/[host].key|.cert
      # - host: example.org
      #  key: put base64 encoded key here
      #  cert: put base64 encoded certificate here
    config:
      # content of apigee config. Make sure that the whole content has the correct indent of two spaces!
      # edge_config, analytics and oauth is already defined
      edgemicro:
      port: 8080
      max_connections: 1000
      max_connections_hard: 5000
      max_times: 300
      config_change_poll_interval: 86400
      logging:
        to_console: true
        level: debug
        stack_trace: false
      plugins:
        sequence:
          - cors-oneapi
          - spikearrest
          #- introspection
          # ApiKey Security needs 'oauth' plugin. Confusing. I know.
          - oauth
          #- quota
          #- app-to-header
          - backend-basicauth
          #- backend-jwt
      proxies:
        # References an Apigee Proxy Configuration
        ###################################
        # !!! REPLACE WITH YOUR PROXY !!! #
        ###################################
        - edgemicro_malwarescanapi_v1
      # In case a proxy is needed for accessing the API backend (target-server)
      proxy:
        url: ""
        enabled: false
    headers:
      x-forwarded-for: true
      x-forwarded-host: true
      x-request-id: true
      x-response-time: true
      via: true
    backend-basicauth:
      username: 'admin'
      password: 'password123'
    cors-oneapi:
      cors-allow-credentials: true
    backend-jwt:
      header_attribute_name: x-claims
      sign_secret: 'my-secret'
      claims:
        - iss
        - sub
        - client_id
        - scope
        - app_name
        - custom_client_identification
    spikearrest:
      timeUnit: minute
      allow: 6000
      bufferSize: 600
    # client certificate configuration
    # targets:
    #   - host: 'example.org'
    #     ssl:
    #       client:
    #         key: /home/node/certs/example.org.key # Don't change this, will be set via certs.key
    #         cert: /home/node/certs/example.org.crt # Don't change this, will be set via certs.cert
    #         passphrase: 'optional'

  image:
    repository: edgemicro
    tag: latest
    pullPolicy: IfNotPresent
  
  nameOverride: ""
  fullnameOverride: ""

  service:
    type: NodePort
    port: 80
    nodePort: 30005

  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # Optionaly an ingress route can be defined. Routing options are configured in here.
  ingress:
    enabled: true
    basePath: /malware-scan/api/v1
    # Define a list of hosts for the routing. If an empty list is provided routing will be enabled for all hostnames
    hosts: [""]
    annotations: {}
    #traefik.ingress.kubernetes.io/rewrite-target: /malware_scan_api
    # Add custom labels to ingress route
    labels: {}

#Subchart properties for vault
vault:
  enabled: true 
  namespace: vault
  global:
    # enabled is the master enabled switch. Setting this to true or false
    # will enable or disable all the components within this chart by default.
    enabled: true
    # Image pull secret to use for registry authentication.
    imagePullSecrets:
    # imagePullSecrets:
       - name: dockerregistry
    # TLS for end-to-end encrypted transport
    tlsDisable: true

  server:
    # Resource requests, limits, etc. for the server cluster placement. This
    # should map directly to the value of the resources field for a PodSpec.
    # By default no direct resource request is made.

    image:
      repository: vault
      tag: latest
      # Overrides the default Image Pull Policy
      pullPolicy: IfNotPresent

    resources:
    # resources:
    #   requests:
    #     memory: 256Mi
    #     cpu: 250m
    #   limits:
    #     memory: 256Mi
    #     cpu: 250m

    # Ingress allows ingress services to be created to allow external access 
    # from Kubernetes to access Vault pods.
    ingress:
      enabled: false
      labels: { }
      # traffic: external
      annotations: { }
        # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      hosts:
        - host: chart-example.local
          # As of now vault can only be servered on "/"
          paths: [ / ]

      tls: [ ]
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local


    # authDelegator enables a cluster role binding to be attached to the service
    # account.  This cluster role binding can be used to setup Kubernetes auth
    # method.  https://www.vaultproject.io/docs/auth/kubernetes.html
    authDelegator:
      enabled: false

    # extraContainers is a list of sidecar containers. Specified as a raw YAML string.
    extraContainers: null


    # Used to define custom readinessProbe settings
    readinessProbe:
      enabled: true
      # If you need to use a http path instead of the default exec
      # path: /v1/sys/health?standbyok
    # Used to enable a livenessProbe for the pods
    livenessProbe:
      enabled: false
      path: /v1/sys/health?standbyok

    # extraEnvironmentVars is a list of extra enviroment variables to set with the stateful set. These could be
    # used to include variables required for auto-unseal.
    extraEnvironmentVars: { }
      # GOOGLE_REGION: global
      # GOOGLE_PROJECT: myproject
    # GOOGLE_APPLICATION_CREDENTIALS: /vault/userconfig/myproject/myproject-creds.json

    # extraSecretEnvironmentVars is a list of extra enviroment variables to set with the stateful set.
    # These variables take value from existing Secret objects.
    extraSecretEnvironmentVars: [ ]
      # - envName: AWS_SECRET_ACCESS_KEY
      #   secretName: vault
    #   secretKey: AWS_SECRET_ACCESS_KEY

    # extraVolumes is a list of extra volumes to mount. These will be exposed
    # to Vault in the path `/vault/userconfig/<name>/`. The value below is
    # an array of objects, examples are shown below.
    extraVolumes: [ ]
      # - type: secret (or "configMap")
      #   name: my-secret
    #   path: null # default is `/vault/userconfig`

    # Affinity Settings
    # Commenting out or setting as empty the affinity variable, will allow
    # deployment to single node services such as Minikube
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: {{ template "vault.name" . }}
                app.kubernetes.io/instance: "{{ .Release.Name }}"
                component: server
            topologyKey: kubernetes.io/hostname

    # Toleration Settings for server pods
    # This should be a multi-line string matching the Toleration array
    # in a PodSpec.
    tolerations: { }

    # nodeSelector labels for server pod assignment, formatted as a muli-line string.
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    # Example:
    # nodeSelector: |
    #   beta.kubernetes.io/arch: amd64
    nodeSelector: { }

    # Extra labels to attach to the server pods
    # This should be a multi-line string mapping directly to the a map of
    # the labels to apply to the server pods
    extraLabels: { }

    # Extra annotations to attach to the server pods
    # This should be a multi-line string mapping directly to the a map of
    # the annotations to apply to the server pods
    annotations: { }

    # Enables a headless service to be used by the Vault Statefulset
    service:
      enabled: true
      # clusterIP controls whether a Cluster IP address is attached to the
      # Vault service within Kubernetes.  By default the Vault service will
      # be given a Cluster IP address, set to None to disable.  When disabled
      # Kubernetes will create a "headless" service.  Headless services can be
      # used to communicate with pods directly through DNS instead of a round robin
      # load balancer.
      # clusterIP: None

      # Port on which Vault server is listening
      port: 8200
      # Target port to which the service should be mapped to
      targetPort: 8200
      # Extra annotations for the service definition
      annotations: { }

    # This configures the Vault Statefulset to create a PVC for data
    # storage when using the file backend.
    # See https://www.vaultproject.io/docs/configuration/storage/index.html to know more
    dataStorage:
      enabled: true
      # Size of the PVC created
      size: 1Gi
      # Name of the storage class to use.  If null it will use the
      # configured default Storage Class.
      storageClass: ""
      # Access Mode of the storage device being used for the PVC
      accessMode: ReadWriteOnce

    # This configures the Vault Statefulset to create a PVC for audit
    # logs.  Once Vault is deployed, initialized and unseal, Vault must
    # be configured to use this for audit logs.  This will be mounted to
    # /vault/audit
    # See https://www.vaultproject.io/docs/audit/index.html to know more
    auditStorage:
      enabled: true
      # Size of the PVC created
      size: 1Gi
      # Name of the storage class to use.  If null it will use the
      # configured default Storage Class.
      storageClass: ""
      # Access Mode of the storage device being used for the PVC
      accessMode: ReadWriteOnce

    # Run Vault in "dev" mode. This requires no further setup, no state management,
    # and no initialization. This is useful for experimenting with Vault without
    # needing to unseal, store keys, et. al. All data is lost on restart - do not
    # use dev mode for anything other than experimenting.
    # See https://www.vaultproject.io/docs/concepts/dev-server.html to know more
    dev:
      enabled: false

    # Run Vault in "standalone" mode. This is the default mode that will deploy if
    # no arguments are given to helm. This requires a PVC for data storage to use
    # the "file" backend.  This mode is not highly available and should not be scaled
    # past a single replica.
    standalone:
      enabled: "-"

      # config is a raw string of default configuration when using a Stateful
      # deployment. Default is to use a PersistentVolumeClaim mounted at /vault/data
      # and store data there. This is only used when using a Replica count of 1, and
      # using a stateful set. This should be HCL.
      config: |
        ui = true

        listener "tcp" {
          tls_disable = 1
          address = "[::]:8200"
          cluster_address = "[::]:8201"
        }
        storage "file" {
          path = "/vault/data"
        }

        # Example configuration for using auto-unseal, using Google Cloud KMS. The
        # GKMS keys must already exist, and the cluster must have a service account
        # that is authorized to access GCP KMS.
        #seal "gcpckms" {
        #   project     = "vault-helm-dev"
        #   region      = "global"
        #   key_ring    = "vault-helm-unseal-kr"
        #   crypto_key  = "vault-helm-unseal-key"
        #}

    # Run Vault in "HA" mode. There are no storage requirements unless audit log
    # persistence is required.  In HA mode Vault will configure itself to use Consul
    # for its storage backend.  The default configuration provided will work the Consul
    # Helm project by default.  It is possible to manually configure Vault to use a
    # different HA backend.
    ha:
      enabled: false
      replicas: 3

      # config is a raw string of default configuration when using a Stateful
      # deployment. Default is to use a Consul for its HA storage backend.
      # This should be HCL.
      config: |
        ui = true

        listener "tcp" {
          tls_disable = 1
          address = "[::]:8200"
          cluster_address = "[::]:8201"
        }
        storage "consul" {
          path = "vault"
          address = "HOST_IP:8500"
        }

        # Example configuration for using auto-unseal, using Google Cloud KMS. The
        # GKMS keys must already exist, and the cluster must have a service account
        # that is authorized to access GCP KMS.
        #seal "gcpckms" {
        #   project     = "vault-helm-dev-246514"
        #   region      = "global"
        #   key_ring    = "vault-helm-unseal-kr"
        #   crypto_key  = "vault-helm-unseal-key"
        #}

      # A disruption budget limits the number of pods of a replicated application
      # that are down simultaneously from voluntary disruptions
      disruptionBudget:
        enabled: true

        # maxUnavailable will default to (n/2)-1 where n is the number of
        # replicas. If you'd like a custom value, you can specify an override here.
        maxUnavailable: null

    # Definition of the serviceAccount used to run Vault.
    serviceAccount:
      annotations: { }

  # Vault UI
  ui:
    # True if you want to create a Service entry for the Vault UI.
    #
    # serviceType can be used to control the type of service created. For
    # example, setting this to "LoadBalancer" will create an external load
    # balancer (for supported K8S installations) to access the UI.
    enabled: false
    serviceType: "ClusterIP"
    serviceNodePort: null
    externalPort: 8200

    # loadBalancerSourceRanges:
    #   - 10.0.0.0/16
    #   - 1.78.23.3/32

    # loadBalancerIP:

    # Extra annotations to attach to the ui service
    # This should be a multi-line string mapping directly to the a map of
    # the annotations to apply to the ui service
    annotations: { }

#subchart  value for  storageMfe
storagemfe:
  enabled: true 
  namespace: storage
  configJs: |
    window["STORAGE_INJECTED_ENVIRONMENT"]={
        CONTAINER_APP_URL: "http:localhost:8080",
        STORAGE_API_BASEURL: "http:localhost:7175/api",
        TOU_HTML: "<div>I agree to <a href=\"https://social/docs/DOC-467646\" target=\"_blank\" rel=\"noopener noreferrer\">terms of use</a></div>", 
        ENABLE_DATA_CLASSIFICATION_SECRET: false,
        API_BASEURL: "http://localhost:8080/api"
        TRINO_API_BASEURL: "http://localhost:7575/api"
      }; 
  conf:
    storageBackendUrl: http://storage-be.storage.svc.cluster.local:80 



  replicaCount: 1

  image:
    #Build & update the storage-frontend service in the docker-compose-local-basic.yml
    repository: dna/storage-frontend
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: latest


  imagePullSecrets:
    - name: dockerregistry

  nameOverride: ""
  fullnameOverride: "storage-mfe"

  podSecurityContext:
    runAsUser: 1001

  containerPort: 3000
  service:
    type: ClusterIP
    port: 80
  
  ingress:
    enabled: false
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: Prefix
    tls:
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
      - hosts:
        - chart-example.local
        secretName: my-tls-secret

storagebe:
  enabled: true
    # Default values for storage-be.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  namespace: storage
  #Add new env as array value with name/value field. Keep one tab indentation.
  envs:
    - name: MAX_FILE_SIZE
      value: 3000MB
    - name: MAX_REQUEST_SIZE
      value: 3000MB
    - name: VAULT_HOST
      value: vault.vault.svc.cluster.local
    - name: VAULT_PORT
      value: "8200"         
    - name: VAULT_SCHEME
      value: http
    - name: VAULT_AUTHENTICATION
      value: TOKEN
    - name: VAULT_TOKEN
      valueFrom:
        secretKeyRef:
          key: vaultToken
          name: storage-be
    - name: VAULT_MOUNTPATH
      value: kv
    - name: VAULT_PATH
      value: dna/minio  
    - name: DNA_URI
      value: http://dna-service.dna.svc.cluster.local:80
    - name: DNA_AUTH_ENABLE
      value: "true"  
    - name: JWT_SECRET_KEY
      valueFrom:
        secretKeyRef:
          key: jwt.secret.key
          name: storage-be
    - name: CORS_ORIGIN_URL
      value: http://* 
    - name: MINIO_ENDPOINT
      value: http://minio.storage.svc.cluster.local:9000
      #The minio_client_api is a coming soon feature
    - name: MINIO_CLIENT_API
      value: ""
    - name: MINIO_ADMIN_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          key: minioAccessKey
          name: storage-be  
    - name: MINIO_ADMIN_SECRET_KEY
      valueFrom:
        secretKeyRef:
          key: minioSecretKey
          name: storage-be   
    - name: MINIO_POLICY_VERSION
      value: "2012-10-17"   
    - name: LOGGING_ENVIRONMENT
      value: DEV   
    - name: LOGGING_PATH
      value: /tmp/storage/log/
    - name: ATTACHMENT_MALWARE_SCAN
      value: "true"  
    - name: MALWARE_SCANNER_URI
      value: http://localhost:9002/api/v1
    - name: MALWARE_SCANNER_APP_ID
      valueFrom:
        secretKeyRef:
          key: malwareApiId
          name: storage-be  
    - name: MALWARE_SCANNER_API_KEY
      valueFrom:
        secretKeyRef:
          key: malwareApiKey
          name: storage-be
    - name: API_DB_USER
      valueFrom:
        secretKeyRef:
          key: dbUser
          name: storage-be
    - name: API_DB_PASS
      valueFrom:
        secretKeyRef:
          key: dbPassword
          name: storage-be
    - name: API_DB_URL
      value: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/storage

      #Update the naas Broker FQDN in the "NAAS_BROKER" parameter
    - name: NAAS_BROKER
      value: 

    - name: STORAGE_TERMS_OF_USE_URL
      value: ""
    - name: DATAIKU_PROD_URI
      value: ""    
    - name: DATAIKU_PROD_API_KEY
      valueFrom:
        secretKeyRef:
          key: dataikuProdApiKey
          name: storage-be
    - name: DATAIKU_TRAINING_URI
      value: ""
    - name: DATAIKU_TRAINING_API_KEY
      valueFrom:
        secretKeyRef:
          key: dataikuTrainingApiKey
          name: storage-be  
  #Update the root token of the vault which you have generated by Initializing the vault to the below "vaultToken" parameter.
  secret:
    - key: vaultToken
      value: ""
    - key: jwt.secret.key 
      value: 
    - key: minioAccessKey 
      value: 
    - key: minioSecretKey
      value: 
    - key: malwareApiId 
      value: ""
    - key: malwareApiKey
      value: ""
    - key: dbUser
      value: storage
    - key: dbPassword
      value: storage123
      #The below dataiku are coming soon features
    - key: dataikuProdApiKey
      value: ""
    - key: dataikuTrainingApiKey
      value: ""  

  replicaCount: 1
  #Build & Update the storage-backend service in the docker-compose-local-basic.yml
  image:
    repository: dna/storage-backend
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: latest

  imagePullSecrets:
    - name: dockerregistry
  nameOverride: ""
  fullnameOverride: "storage-be"

  podSecurityContext:
    #We are running this applicaton as root  
    runAsUser: 1001
    

  containerPort: 7175
  service:
    type: ClusterIP
    port: 80

  ingress:
    enabled: false
    annotations:
      kubernetes.io/ingress.class: nginx
      kubernetes.io/tls-acme: "true"
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
    hosts:
      - host: chart-example.local
        paths:
          - path: /storage/api
            pathType: Prefix
          - path: /storage/swagger-ui.html
            pathType: Prefix
          - path: /storage/swagger-resources
            pathType: Prefix
          - path: /storage/v2/api-docs
            pathType: Prefix
          - path: /storage/webjars
            pathType: Prefix
    tls:
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
      - hosts:
        - chart-example.local
        secretName: my-tls-secret

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
minio:
  enabled: true

#Subchart properties for model-registry

model-registry:
  enabled: false

#subChart properties for trinoBackend
trinoBackend:
  enabled: false