# Default values for helm.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

#Suchart properties for frontend
frontend:
  enabled: true
  namespace: dna
  imagePullSecret: 
    name: dockerregistry
    key: "docker-config" 
  ingress:
    enabled: true
    namespace: ingress
    host: "localhost"
    lbIP: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  appFrontend:
    replicaCount: 1
    #Specify the frontend image, after building the image from docker file
    image: vardhandevalla/dna-app-frontend01:latest
    config:
    #If you want to enable OIDC authentication then set oidcDisabled to "false"
      oidcDisabled: true
      oidcProvider: INTERNAL
      apiBaseUrl: api
    #Specify the Jupyter Notebook url 
      jupyterNotebookUrl: http://localhost/notebooks
      jupyterNotebookOidcPopUpUrl: http://localhost/notebooks/hub/oauth_login?next=
    #Specify the dataiku live application URL
      dataikuLiveAppUrl: ""
      dataikuTrainingAppUrl: ""
    #Specify the OAuth token URL for OIDC authentication
      oauth2TokenUrl: ""
      oauth2AuthUrl: ""
      oauth2RevokeUrl: ""
      oauth2LogoutUrl: ""
      oauth2IntrospectionUrl: ""
      oauth2UserInfoUrl: ""
      frontendClientid: ""
      redirectUrls: ""


    #Specify the swagger URL for malwarescan
      swaggerUiUrl: http://localhost/avscan/swagger-ui.html#/
      dataikuFerretUrl: ""
    #Specify the ML pipeline URL
      mlPipelineUrl: ""
    #Specify the App Header
      appNameHeader: ""
      appNameHome: ""
      contactUsHtml: <div><p>There could be many places where you may need our help, and we are happy to support you. <br /> Please add your communication channels links here</p></div>
      brandLogoUrl: /images/branding/logo-brand.png
      appLogoUrl: /images/branding/logo-app.png
      enableInternalUserInfo: true
      enableDataCompliance: true
    # Set enabledReports to "true" if you want to enable reports, by default value is "false"
      enabledReports: true
    # Set enableJupyterWorkspace to "true", if you want to enable Jupyter Workspace
      enableJupyterWorkspace: false
    # Set enableDataikuWorkspace to "true", if you want to enable Dataiku Workspace, by default value is "false"
      enableDataikuWorkspace: false
      enableMalwareService: true
      enableDataPipelineService: false
    # Set enableStorageService to "true" to enable Storage Service
      enableStorageService: true
      storageMFEAppURL: http://localhost:7175
      enablePipelineService: false
      enabledMlPipelineService: false
      enableMalwareApiInfo: false
    # If you want to enable notification, set enableNotification to "true", by default it is false
      enableNotification: true
    # Specify the company name 
      companyName: XYZ
      backendHost: dna-service

  # Configure ngnix as per the 
    ngnix:
      backend: http://dna-service.dna.svc.cluster.local:80
      jupyServer: http://proxy-public:8000
      grafanaServer: http://i3-monitoring-grafana.i3-monitoring.svc.cluster.local:80
      avscanServer: http://clamav-rest-service.clamav.svc.cluster.local:8181
      avscanMgwServer: http://dna-microgateway.clamav.svc.cluster.local:80
      airflowServer: http://airflow.airflow.svc.cluster.local:8080
      naasServer: http://naas-backend-service.naas.svc.cluster.local:7272
      dashboard: http://dashboard-backend-service.dashboard.svc.cluster.local:7173
      storage: http://storage-be.storage.svc.cluster.local:80      

#Suchart properties for backend
backend:
  enabled: true
  namespace: dna
  imagePullSecret: 
    name: dockerregistry
    key: "docker-config" 
  ingress:
    enabled: true
    namespace: ingress
    host: "ingress-host"
    lbIP: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  app:
    backend:
      replicaCount: 1
      
      image: vardhandevalla/dna-app-backend:latest
      #Specify the secrets which will be used by backend to run properly, Secrets will be encoded in base64 at the time of deployment
      secrets: 
        name: app-secrets
        notebookSecretToken: ""
        oidcClientID: ""
        oidcClientSecret: "" 
        drdCertPassword: ""
        jwtSecretKey: eyJhbGciOiJIUzI1NiJ9.eyJSb2xlIjoiQWRtaW4iLCJJc3N1ZXIiOiJJc3N1ZXIiLCJVc2VybmFtZSI6IkphdmFJblVzZSIsImV4cCI6MTY0OTY2Mzg5MCwiaWF0IjoxNjQ5NjYzODkwfQ.0bHIjToqnWk5zOq0a-Bn-HV6jw6-bnVCNx56L5QnVkg
        s3AccessKey: admin
        s3SecretKey: S6O!#uiOop
        dataikuProdApiKey: ""
        dataikuTrainingApiKey: "" 
        avscanApiKey: ""
        appDBUserName: postgres
        appDBPassword: postgres
      config:
        enableItsmm: false
        enableJupyterNotebook: false
        enableDataiku: false
        enableAttachmentScan: false
        enableInternalUser: false
        redirectUrl: ""
        dbUri: jdbc:postgresql://dna-bitnamipostgresql:5432/db
        oidcUserInfoUrl: "" 
        oidcTokenIntrospectionUrl: "" 
        oidcProvider: INTERNAL
        oidcTokenRevocationUrl: ""
        internalUserRequestUrl: ""
        internalCertFile: ""
        oidcDisabled: true
        s3BucketName: dna
        s3Url: http://minio.storage.svc.cluster.local:9000
        corosOriginUrl: http://localhost:7179
        jupyterNotebookUrl: "" 
        vaultHost: vault.vault.svc.cluster.local
        vaultPort: 8200
        dataikuProdUri: ""
        dataikuProdAdminGroup: "" 
        dataikutraininguri: ""
        avscanAppId: ""
        naasBroker: kafka-headless.kafka.svc.cluster.local:9092
        loggingEnvironment: dev
        loggingPath: /tmp/app/log
        dataikuProjectUri: /projects/
        dataikuTrainingAdminGroup: ""
        avscanUri: ""
        flywayBaseline: "true"
        flywayBaselineOnMigrate: "true"
        flywayBaselineVersion: "0"
        flywaySchema: "public"
        dashboardUri: http://dashboard-backend-service.dashboard.svc.cluster.local:7173/dashboards

    vault:
      secret:
        name: vault-secrets
        rootToken: ***REMOVED***

#Suchart properties for i3postgressql database
i3postgresql:
  enabled: false
  namespace: dna
  app:
    db:
      replicaCount: 1
      image: postgresql:10.3-2.1.0
      pgssl: "NO"
      dbname: db
      secrets: 
        name: postgres-secrets
        backupUserName: ""
        backupUserPassword: ""
        dbAdminUserName: ""
        dbAdminPassword: ""
        patroniUserPassword: ""
        appUserName: ""
        appPassword: ""
      airflowDB:
        airflowUserName: ""
        airflowdbName: ""
        airflowdbPwd: ""
      dashboardDB:
        dashboardUserName: ""
        dashboarddbName: ""
        dashboarddbPwd: "" 

#Subchart properties for bitnami postgresql
bitnamipostgresql:
  enabled: true
  namespace: dna
  global:
    postgresql:
      postgresqlDatabase: db
      postgresqlUsername: postgres
      postgresqlPassword: postgres
      servicePort: 5432

#Subchart properties for clamav
clamav:
  enabled: true
  appName: clamav
  namespace: clamav
  app:
    backend:
      name: clamav-rest
      image: vardhandevalla/malware-backend:latest
      replicaCount: 1
      secrets:
        name: oneapi-secrets
        onapiBasicAuthToken: ""
      config: 
        clamav_backend_url: clamav-service
        clamav_backend_port: 3310
        max_file_size: 10MB
        max_request_size: 11MB
        api_request_limit: 20
        with_in: 2
        time_unit: seconds
        auth_api_host: http://dna-service.dna.svc.cluster.local:80/api/subscription/validate
        restricted_url_pattern: /avscan/api/v1/scan.*
        loggingPath: /tmp/clamav/log
        loggingEnvironment: dev
        corsOriginUrl: ""

    probes:
      initialDelaySeconds: 60
      timeoutSeconds: 10
      periodSeconds: 10
      failureThreshold: 3
      livenessProbe:
        path: /avscan/actuator/health/liveness
        port: api
      
      readinessProbe:
        path: /avscan/actuator/health/readiness
        port: api
  image:
    repo: vardhandevalla/clamav:latest
    replicaCount: 1
    pullPolicy: Always
  ingress:
    enabled: true
    host: "" 
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: "" 
  imagePullSecret: 
    name: dockerregistry
    key: "docker-config" 

  Storage:
    storageClass: managed
    accessModes: ReadWriteOnce
    size: 2G

  # resources:
  #   requests:
  #     memory: "512Mi"
  #     cpu: "250m"
  #   limits:
  #     memory: "1000Mi"
  #     cpu: "500m"
      
#Subchart properties for naas
naas:
  enabled: true
  appName: naas
  namespace: naas
  app:
    backend:
      image: vardhandevalla/naas-backend:latest
      secrets:
        name: naas-app-secrets
        authApiToken: ""
        jwtKey: eyJhbGciOiJIUzI1NiJ9.eyJSb2xlIjoiQWRtaW4iLCJJc3N1ZXIiOiJJc3N1ZXIiLCJVc2VybmFtZSI6IkphdmFJblVzZSIsImV4cCI6MTY0OTY2Mzg5MCwiaWF0IjoxNjQ5NjYzODkwfQ.0bHIjToqnWk5zOq0a-Bn-HV6jw6-bnVCNx56L5QnVkg
        db: 
          appUserName: postgres
          appPassword: postgres
      config:
        api_db_url: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/db
        naas_broker: kafka-headless.kafka.svc.cluster.local:9092
        max_poll_records: 6000
        dna_uri: http://dna-service.dna.svc.cluster.local:80
        dna_auth_enable: false
        mailServerHost: ""
        mailServerPort: ""
        notificationSenderEmail: "" 
        poll_time: 5000
        naas_central_topic: CentralEventTopic
        naas_centralread_topic: CentralReadTopic
        naas_centraldelte_topic: CentralDeleteTopic
        loggingPath: /tmp/naas/log
        loggingEnvironment: dev
    resources:
      requests:
        memory: "512Mi"
        cpu: "250m"
      limits:
        memory: "1000Mi"
        cpu: "500m"
    probes:
      initialDelaySeconds: 180
      timeoutSeconds: 10
      periodSeconds: 30
      failureThreshold: 3
      livenessProbe:
        path: /naas/actuator/health/liveness
        port: api
      
      readinessProbe:
        path: /naas/actuator/health/readiness
        port: api
  
  image:
  pullPolicy: Always
      
  ingress:
    enabled: true
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  imagePullSecret: 
   name: dockerregistry
   key: "docker-config" 
  
#Subchart properties for notebooks
notebooks:
  enabled: false
  namespace: notebooks
  app:
    jupyter:
      config:
        configproxy_auth_token: ""
        kf_pipelines_endpoint: http://ml-pipeline-ui.kubeflow
    image:
      name: jupyterhub:1.0
      pullPolicy: Always
    
    profileListImages:
      default: pyspark-notebook:1.0-default
      tensorflow: pyspark-notebook:1.0-tensorflow
      chronos: pyspark-notebook:1.0-chronos    

    proxy:
      image: configurable-http-proxy:latest
      
    
    hubConfig:
      name: hub-config
      KubeSpawnerimage: pyspark-notebook:1.0-default
      securitycontext: ""
      serviceAccount: "hub"
      oauthAuthenticator: GenericOAuthenticator
      oauthClientId: ""
      oauthClientSecret: ""
      oauthCallback: ""
      oauthAuthorizeUrl: ""
      oauthTokenUrl: ""
      oauthUserDataUrl: ""
      oauthUsrKey: sub
      oauthLoginSvc: OIDC
      prespawn_hook: ""
      enableUserNS: "False"
      userNameSpaceTemplate: kubeflow

  ingress:
    enabled: true
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""

  Storage:
    storageClass: cinder
    accessModes: ReadWriteOnce
    size: 1G
  
  resources:
    cpu: 1
    memory: 4G

#Subchart properties for dashboard
dashboard:
  enabled: true
  namespace: dashboard
  image: vardhandevalla/dashboard-backend
  imagePullSecret: 
    name: dockerregistry
    key: "docker-config" 
  dbUrl: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/dashboard
  secret:
    name: dashboard-secrets
    appUserName: dashboard
    appPassword: dashboard
  
    jwtKey: eyJhbGciOiJIUzI1NiJ9.eyJSb2xlIjoiQWRtaW4iLCJJc3N1ZXIiOiJJc3N1ZXIiLCJVc2VybmFtZSI6IkphdmFJblVzZSIsImV4cCI6MTY0OTY2Mzg5MCwiaWF0IjoxNjQ5NjYzODkwfQ.0bHIjToqnWk5zOq0a-Bn-HV6jw6-bnVCNx56L5QnVkg
  appUrl: http://dna-service.dna.svc.cluster.local:80
  enableAuth: false
  loggingPath: /tmp/dashboard/log
  loggingEnvironment: dev
  flywayBaseline: "true"
  flywayBaselineOnMigrate: "true"
  flywayBaselineVersion: 0
  flywaySchema: "public"
  containerPort: 7173
  ingress:
    enabled: true
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1000Mi"
      cpu: "500m"

  probes:
    initialDelaySeconds: 120
    timeoutSeconds: 10
    periodSeconds: 20
    failureThreshold: 3
    livenessProbe:
      path: /dashboards/actuator/health/liveness
      port: api
        
    readinessProbe:
      path: /dashboards/actuator/health/readiness
      port: api

#Subchart properties for airflow
airflow:
  enabled: false
  appName: airflow
  namespace: airflow
  imagePullSecret: 
    name: dockerregistry
    key: "docker-config" 
  backend:
    image: vardhandevalla/airflow-backend-2
    imagePullPolicy: Always
    dbUrl: jdbc:postgresql://dna-bitnamipostgresql.dna.svc.cluster.local:5432/airflow
    secret:
      name: airflow-backend-secrets
      dbPassword: airflow
      dbUsername: airflow
      gitToken: ***REMOVED***
      jwtKey: eyJhbGciOiJIUzI1NiJ9.eyJSb2xlIjoiQWRtaW4iLCJJc3N1ZXIiOiJJc3N1ZXIiLCJVc2VybmFtZSI6IkphdmFJblVzZSIsImV4cCI6MTY0OTY2Mzg5MCwiaWF0IjoxNjQ5NjYzODkwfQ.0bHIjToqnWk5zOq0a-Bn-HV6jw6-bnVCNx56L5QnVkg
      oidcClientID: ""
      oidcClientSecret: ""
    containerPort: 7171
    crossOriginUrl: ""
    apiUrl: http://dna-service.dna.svc.cluster.local:80
    oidcInfoUrl: "" 
    oidcIntrospectionUrl: "" 
    oidcRevocationUrl: "" 
    oidcDisabled: true
    gitUrl: https://vardhandevalla:***REMOVED***@github.com/Vardhandevalla/airflow_dags.git
    gitMountPath: /git/airflow-user-dags
    gitBranch: main
    dag:
      path: dags
      ext: py
      waitTime: 20
      retry: 20
    loggingPath: /tmp/airflow/log
    loggingEnvironment: dev
    flywayBaseline: "true"
    flywayBaselineOnMigrate: "true"
    flywayBaselineVersion: "0"
    flywaySchema: "public"

  ingress:
    enabled: true
    host: ""
    annotations:
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""

  pullSecretData: eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOnsidXNlcm5hbWUiOiJ2YXJkaGFuZGV2YWxsYSIsInBhc3N3b3JkIjoibXZhZ3VzdGFARjQiLCJlbWFpbCI6InZhcmRoYW5kZXZhbGxhQGdtYWlsLmNvbSIsImF1dGgiOiJkbUZ5WkdoaGJtUmxkbUZzYkdFNmJYWmhaM1Z6ZEdGQVJqUT0ifX19
  secret:
    gitUserName: vardhandevalla
    gitPassword: ***REMOVED***
    #gitSshKey: ""
    knownHosts: ""
    postgresql:
      sqlAlchemyConn: postgresql+psycopg2://airflow:airflow@dna-bitnamipostgresql.dna.svc.cluster.local:5432/airflow
    #clientSecret: eyJSb2xlIjoiQWRtaW4iLCJJc3N1ZXIiOiJJc3N1ZXIiLCJVc2VybmFtZSI6IkphdmFJblVzZSIsImV4cCI6MTY0OTY2Mzg5MCwiaWF0IjoxNjQ5NjYzODkwfQ.0bHIjToqnWk5zOq0a-Bn-HV6jw6-bnVCNx56L5QnVkg

  gitSync:
    image: vardhandevalla/airflow-git-sync:latest
    repo: https://github.com/Vardhandevalla/airflow_dags.git
    dest: git-sync
    branch: main
    ssh: "false"
    root: /git

  # editor:
  #   gitEnabled: True
  #   gitCMD: /usr/bin/git
  #   gitDefaultArgs: -c color.ui=true
  #   gitIntRepo: False
  #   lineLength: 88
  #   stringNormalization: False
    
  configuration:
    loggingLevel: INFO
    executor: KubernetesExecutor
    parallelism: 32
    pluginsFolder: /usr/local/airflow/plugins
    loadExamples: False
    scheduler:
      dagDirListInterval: 5
      childProcessLogDirectory: /usr/local/airflow/logs/scheduler
      jobHeartbeatSec: 5
      parsingProcesses: 2
      schedulerHeartbeatSec: 5
      minFileProcessInterval: 0
      statsdOn: False
      statsdHost: localhost
      statsdPort: 8125
      statsdPrefix: airflow
      minFileParsingLoopTime: 1
      printStatsInterval: 30
      schedulerZombieTaskThreshold: 300
      maxTisPerQuery: 0
      authenticate: False
      catchupByDefault: True
    webserver:
      baseUrl: ""
      path: /pipelines
      rbac: True
      host: 0.0.0.0
      port: 8080
      masterTimeout: 120
      workerTimeout: 120
      workerRefreshBatchSize: 1
      workerRefreshInterval: 30
      secretKey: ""
      numberOfWorkers: 4
      workerClass: sync
      exposeConfig: True
      dagDefaultView: graph
      dagOrientation: LR
      demoMode: False
      logFetchTimeoutSec: 5
      hidePausedDagsByDefault: False
      pageSize: 100
    kubernetes:
      workerContainerImagePullPolicy: Always
      workerServiceAccountName: airflow
      deleteWorkerPods: True
      dagsInImage: false
      gitSubpath: dags
      inCluster: True
      gitSyncContainerRepository: vardhandevalla/airflow-git-sync
      gitSyncContainerTag: latest
      gitSyncInitContainerName: git-sync-container
      gitSyncRunAsUser: 1000
      runAsUser: 1000
      fsGroup: 65533
    kubernetesLabels:
      airflowWorker:

  docker:
    image:
      name: vardhandevalla/airflow-frontend-4
      tag: latest

  service:
    port: 8080

  db:
    port: 64000

  webserver:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 2000Mi
      cpu: 1000m

  scheduler:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 2000Mi
      cpu: 1000m

  gitContainer:
    requests:
      memory: 250Mi
      cpu: 250m
    limits:
      memory: 1000Mi
      cpu: 500m

  backendResources:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 1000Mi
      cpu: 500m

  volumes:
    logsPath: /usr/local/airflow/logs
    dagsPath: /usr/local/airflow/dags/git-sync/dags
    gitDagsPath: /usr/local/airflow/dags
    airflowLogsClaim:
      resourcePolicy: keep
      accessMode: ReadWriteOnce
      storage:
        className: managed
        size: 2Gi

  oidc:
    logout:
      uri: ""

  Storage:
    storageClass: managed 
    accessModes: ReadWriteOnce
    size: 1Gi
  
#Subchart properties for microgateway

microgateway:
  enabled: false
  namespace: clamav
  proxy: ""
  noProxy: ""
  apigee:
    # environment variables 
    debug: "*" # Enable DEBUG mode with "*"
    key: ""
    secret: ""
    org: internal
    env: development
    # if introspection is required add the introspection credentials
    #introspection_client_id: 
    #introspection_client_secret: 
    
    # validate target https certificates (1=enabled; 0=disabled)
    node_tls_reject_unauthorized: 1

    certs:
      # key and cert will be mount under /home/node/certs/[host].key|.cert
      # - host: example.org
      #  key: put base64 encoded key here
      #  cert: put base64 encoded certificate here
    config:
      # content of apigee config. Make sure that the whole content has the correct indent of two spaces!
      # edge_config, analytics and oauth is already defined
      edgemicro:
      port: 8080
      max_connections: 1000
      max_connections_hard: 5000
      max_times: 300
      config_change_poll_interval: 86400
      logging:
        to_console: true
        level: debug
        stack_trace: false
      plugins:
        sequence:
          - cors-oneapi
          - spikearrest
          #- introspection
          # ApiKey Security needs 'oauth' plugin. Confusing. I know.
          - oauth
          #- quota
          #- app-to-header
          - backend-basicauth
          #- backend-jwt
      proxies:
        # References an Apigee Proxy Configuration
        ###################################
        # !!! REPLACE WITH YOUR PROXY !!! #
        ###################################
        - edgemicro_malwarescanapi_v1
      # In case a proxy is needed for accessing the API backend (target-server)
      proxy:
        url: ""
        enabled: false
    headers:
      x-forwarded-for: true
      x-forwarded-host: true
      x-request-id: true
      x-response-time: true
      via: true
    backend-basicauth:
      username: 'admin'
      password: 'password123'
    cors-oneapi:
      cors-allow-credentials: true
    backend-jwt:
      header_attribute_name: x-claims
      sign_secret: 'my-secret'
      claims:
        - iss
        - sub
        - client_id
        - scope
        - app_name
        - custom_client_identification
    spikearrest:
      timeUnit: minute
      allow: 6000
      bufferSize: 600
    # client certificate configuration
    # targets:
    #   - host: 'example.org'
    #     ssl:
    #       client:
    #         key: /home/node/certs/example.org.key # Don't change this, will be set via certs.key
    #         cert: /home/node/certs/example.org.crt # Don't change this, will be set via certs.cert
    #         passphrase: 'optional'

  image:
    repository: edgemicro
    tag: latest
    pullPolicy: IfNotPresent
  
  nameOverride: ""
  fullnameOverride: ""

  service:
    type: NodePort
    port: 80
    nodePort: 30005

  resources:
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # Optionaly an ingress route can be defined. Routing options are configured in here.
  ingress:
    enabled: true
    basePath: /malware-scan/api/v1
    # Define a list of hosts for the routing. If an empty list is provided routing will be enabled for all hostnames
    hosts: [""]
    annotations: {}
    #traefik.ingress.kubernetes.io/rewrite-target: /malware_scan_api
    # Add custom labels to ingress route
    labels: {}

#Subchart properties for vault
vault:
  enabled: true 
  namespace: vault
  global:
    # enabled is the master enabled switch. Setting this to true or false
    # will enable or disable all the components within this chart by default.
    enabled: true
    # Image pull secret to use for registry authentication.
    imagePullSecrets:
    # imagePullSecrets:
       - name: dockerregistry
    # TLS for end-to-end encrypted transport
    tlsDisable: true

  server:
    # Resource requests, limits, etc. for the server cluster placement. This
    # should map directly to the value of the resources field for a PodSpec.
    # By default no direct resource request is made.

    image:
      repository: docker.io/vardhandevalla/vault
      tag: latest
      # Overrides the default Image Pull Policy
      pullPolicy: IfNotPresent

    resources:
    # resources:
    #   requests:
    #     memory: 256Mi
    #     cpu: 250m
    #   limits:
    #     memory: 256Mi
    #     cpu: 250m

    # Ingress allows ingress services to be created to allow external access 
    # from Kubernetes to access Vault pods.
    ingress:
      enabled: false
      labels: { }
      # traffic: external
      annotations: { }
        # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      hosts:
        - host: chart-example.local
          # As of now vault can only be servered on "/"
          paths: [ / ]

      tls: [ ]
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local


    # authDelegator enables a cluster role binding to be attached to the service
    # account.  This cluster role binding can be used to setup Kubernetes auth
    # method.  https://www.vaultproject.io/docs/auth/kubernetes.html
    authDelegator:
      enabled: false

    # extraContainers is a list of sidecar containers. Specified as a raw YAML string.
    extraContainers: null


    # Used to define custom readinessProbe settings
    readinessProbe:
      enabled: true
      # If you need to use a http path instead of the default exec
      # path: /v1/sys/health?standbyok
    # Used to enable a livenessProbe for the pods
    livenessProbe:
      enabled: false
      path: /v1/sys/health?standbyok

    # extraEnvironmentVars is a list of extra enviroment variables to set with the stateful set. These could be
    # used to include variables required for auto-unseal.
    extraEnvironmentVars: { }
      # GOOGLE_REGION: global
      # GOOGLE_PROJECT: myproject
    # GOOGLE_APPLICATION_CREDENTIALS: /vault/userconfig/myproject/myproject-creds.json

    # extraSecretEnvironmentVars is a list of extra enviroment variables to set with the stateful set.
    # These variables take value from existing Secret objects.
    extraSecretEnvironmentVars: [ ]
      # - envName: AWS_SECRET_ACCESS_KEY
      #   secretName: vault
    #   secretKey: AWS_SECRET_ACCESS_KEY

    # extraVolumes is a list of extra volumes to mount. These will be exposed
    # to Vault in the path `/vault/userconfig/<name>/`. The value below is
    # an array of objects, examples are shown below.
    extraVolumes: [ ]
      # - type: secret (or "configMap")
      #   name: my-secret
    #   path: null # default is `/vault/userconfig`

    # Affinity Settings
    # Commenting out or setting as empty the affinity variable, will allow
    # deployment to single node services such as Minikube
    affinity: |
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: {{ template "vault.name" . }}
                app.kubernetes.io/instance: "{{ .Release.Name }}"
                component: server
            topologyKey: kubernetes.io/hostname

    # Toleration Settings for server pods
    # This should be a multi-line string matching the Toleration array
    # in a PodSpec.
    tolerations: { }

    # nodeSelector labels for server pod assignment, formatted as a muli-line string.
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
    # Example:
    # nodeSelector: |
    #   beta.kubernetes.io/arch: amd64
    nodeSelector: { }

    # Extra labels to attach to the server pods
    # This should be a multi-line string mapping directly to the a map of
    # the labels to apply to the server pods
    extraLabels: { }

    # Extra annotations to attach to the server pods
    # This should be a multi-line string mapping directly to the a map of
    # the annotations to apply to the server pods
    annotations: { }

    # Enables a headless service to be used by the Vault Statefulset
    service:
      enabled: true
      # clusterIP controls whether a Cluster IP address is attached to the
      # Vault service within Kubernetes.  By default the Vault service will
      # be given a Cluster IP address, set to None to disable.  When disabled
      # Kubernetes will create a "headless" service.  Headless services can be
      # used to communicate with pods directly through DNS instead of a round robin
      # load balancer.
      # clusterIP: None

      # Port on which Vault server is listening
      port: 8200
      # Target port to which the service should be mapped to
      targetPort: 8200
      # Extra annotations for the service definition
      annotations: { }

    # This configures the Vault Statefulset to create a PVC for data
    # storage when using the file backend.
    # See https://www.vaultproject.io/docs/configuration/storage/index.html to know more
    dataStorage:
      enabled: true
      # Size of the PVC created
      size: 1Gi
      # Name of the storage class to use.  If null it will use the
      # configured default Storage Class.
      storageClass: managed
      # Access Mode of the storage device being used for the PVC
      accessMode: ReadWriteOnce

    # This configures the Vault Statefulset to create a PVC for audit
    # logs.  Once Vault is deployed, initialized and unseal, Vault must
    # be configured to use this for audit logs.  This will be mounted to
    # /vault/audit
    # See https://www.vaultproject.io/docs/audit/index.html to know more
    auditStorage:
      enabled: true
      # Size of the PVC created
      size: 1Gi
      # Name of the storage class to use.  If null it will use the
      # configured default Storage Class.
      storageClass: managed
      # Access Mode of the storage device being used for the PVC
      accessMode: ReadWriteOnce

    # Run Vault in "dev" mode. This requires no further setup, no state management,
    # and no initialization. This is useful for experimenting with Vault without
    # needing to unseal, store keys, et. al. All data is lost on restart - do not
    # use dev mode for anything other than experimenting.
    # See https://www.vaultproject.io/docs/concepts/dev-server.html to know more
    dev:
      enabled: false

    # Run Vault in "standalone" mode. This is the default mode that will deploy if
    # no arguments are given to helm. This requires a PVC for data storage to use
    # the "file" backend.  This mode is not highly available and should not be scaled
    # past a single replica.
    standalone:
      enabled: "-"

      # config is a raw string of default configuration when using a Stateful
      # deployment. Default is to use a PersistentVolumeClaim mounted at /vault/data
      # and store data there. This is only used when using a Replica count of 1, and
      # using a stateful set. This should be HCL.
      config: |
        ui = true

        listener "tcp" {
          tls_disable = 1
          address = "[::]:8200"
          cluster_address = "[::]:8201"
        }
        storage "file" {
          path = "/vault/data"
        }

        # Example configuration for using auto-unseal, using Google Cloud KMS. The
        # GKMS keys must already exist, and the cluster must have a service account
        # that is authorized to access GCP KMS.
        #seal "gcpckms" {
        #   project     = "vault-helm-dev"
        #   region      = "global"
        #   key_ring    = "vault-helm-unseal-kr"
        #   crypto_key  = "vault-helm-unseal-key"
        #}

    # Run Vault in "HA" mode. There are no storage requirements unless audit log
    # persistence is required.  In HA mode Vault will configure itself to use Consul
    # for its storage backend.  The default configuration provided will work the Consul
    # Helm project by default.  It is possible to manually configure Vault to use a
    # different HA backend.
    ha:
      enabled: false
      replicas: 3

      # config is a raw string of default configuration when using a Stateful
      # deployment. Default is to use a Consul for its HA storage backend.
      # This should be HCL.
      config: |
        ui = true

        listener "tcp" {
          tls_disable = 1
          address = "[::]:8200"
          cluster_address = "[::]:8201"
        }
        storage "consul" {
          path = "vault"
          address = "HOST_IP:8500"
        }

        # Example configuration for using auto-unseal, using Google Cloud KMS. The
        # GKMS keys must already exist, and the cluster must have a service account
        # that is authorized to access GCP KMS.
        #seal "gcpckms" {
        #   project     = "vault-helm-dev-246514"
        #   region      = "global"
        #   key_ring    = "vault-helm-unseal-kr"
        #   crypto_key  = "vault-helm-unseal-key"
        #}

      # A disruption budget limits the number of pods of a replicated application
      # that are down simultaneously from voluntary disruptions
      disruptionBudget:
        enabled: true

        # maxUnavailable will default to (n/2)-1 where n is the number of
        # replicas. If you'd like a custom value, you can specify an override here.
        maxUnavailable: null

    # Definition of the serviceAccount used to run Vault.
    serviceAccount:
      annotations: { }

  # Vault UI
  ui:
    # True if you want to create a Service entry for the Vault UI.
    #
    # serviceType can be used to control the type of service created. For
    # example, setting this to "LoadBalancer" will create an external load
    # balancer (for supported K8S installations) to access the UI.
    enabled: false
    serviceType: "ClusterIP"
    serviceNodePort: null
    externalPort: 8200

    # loadBalancerSourceRanges:
    #   - 10.0.0.0/16
    #   - 1.78.23.3/32

    # loadBalancerIP:

    # Extra annotations to attach to the ui service
    # This should be a multi-line string mapping directly to the a map of
    # the annotations to apply to the ui service
    annotations: { }

#subchart  value for  storageMfe
storagemfe:
  enabled: true
  namespace: storage
  conf:
    storageBackendUrl: http://storage-be.storage.svc.cluster.local:80 


  envs:
  #Add new env as array value with name/value field. Keep one tab indentation.
    - name: PROJECTSMO_CONTAINER_APP_URL
      value: http://localhost:7179
    - name: PROJECTSMO_FRONTEND_API_BASEURL
      value: api
  replicaCount: 1

  image:
    repository: docker.io/vardhandevalla/storage_frontend_mfe
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: latest


  imagePullSecrets:
    - name: dockerregistry

  nameOverride: ""
  fullnameOverride: "storage-mfe"

  podSecurityContext:
    runAsUser: 1001

  containerPort: 3000
  service:
    type: ClusterIP
    port: 80
  
  ingress:
    enabled: true
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: Prefix
    tls:
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
      - hosts:
        - chart-example.local
        secretName: my-tls-secret

storagebe:
  enabled: false
    # Default values for storage-be.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  namespace: storage
  #Add new env as array value with name/value field. Keep one tab indentation.
  envs:
    - name: MAX_FILE_SIZE
      value: 3000MB
    - name: MAX_REQUEST_SIZE
      value: 3000MB
    - name: VAULT_HOST
      value: vault.vault.svc.cluster.local
    - name: VAULT_PORT
      value: "8200"         
    - name: VAULT_SCHEME
      value: http
    - name: VAULT_AUTHENTICATION
      value: TOKEN
    - name: VAULT_TOKEN
      valueFrom:
        secretKeyRef:
          key: vaultToken
          name: storage-be
    - name: VAULT_MOUNTPATH
      value: kv
    - name: VAULT_PATH
      value: dna/minio  
    - name: DNA_URI
      value: http://dna-service.dna.svc.cluster.local:80
    - name: DNA_AUTH_ENABLE
      value: "true"  
    - name: JWT_SECRET_KEY
      valueFrom:
        secretKeyRef:
          key: jwt.secret.key
          name: storage-be
    - name: CORS_ORIGIN_URL
      value: "" 
    - name: MINIO_ENDPOINT
      value: http://minio.storage.svc.cluster.local:9000
    - name: MINIO_ADMIN_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          key: minioAccessKey
          name: storage-be  
    - name: MINIO_ADMIN_SECRET_KEY
      valueFrom:
        secretKeyRef:
          key: minioSecretKey
          name: storage-be   
    - name: MINIO_POLICY_VERSION
      value: "2012-10-17"   
    - name: LOGGING_ENVIRONMENT
      value: DEV   
    - name: LOGGING_PATH
      value: /tmp/log/   

  secret:
    - key: vaultToken
      value: ***REMOVED***
    - key: jwt.secret.key 
      value: eyJhbGciOiJIUzI1NiJ9.eyJSb2xlIjoiQWRtaW4iLCJJc3N1ZXIiOiJJc3N1ZXIiLCJVc2VybmFtZSI6IkphdmFJblVzZSIsImV4cCI6MTY0OTY2Mzg5MCwiaWF0IjoxNjQ5NjYzODkwfQ.0bHIjToqnWk5zOq0a-Bn-HV6jw6-bnVCNx56L5QnVkg
    - key: minioAccessKey 
      value: admin
    - key: minioSecretKey
      value: S6O!#uiOop

  replicaCount: 1

  image:
    repository: docker.io/vardhandevalla/storage_mfe_backend
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: latest

  imagePullSecrets:
    - name: dockerregistry
  nameOverride: ""
  fullnameOverride: "storage-be"

  podSecurityContext:
    #We are running this applicaton as root  
    runAsUser: 1001
    

  containerPort: 7175
  service:
    type: ClusterIP
    port: 80

  ingress:
    enabled: true
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      traefik.frontend.rule.type: PathPrefix
      kubernetes.io/ingress.class: traefik
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/router.entrypoints: websecure
      cert-manager.io/cluster-issuer: ""
    hosts:
      - host: chart-example.local
        paths:
          - path: /storage/api
            pathType: Prefix
          - path: /storage/swagger-ui.html
            pathType: Prefix
          - path: /storage/swagger-resources
            pathType: Prefix
          - path: /storage/v2/api-docs
            pathType: Prefix
          - path: /storage/webjars
            pathType: Prefix
    tls:
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local
      - hosts:
        - chart-example.local
        secretName: my-tls-secret

  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi
minio:
  enabled: true